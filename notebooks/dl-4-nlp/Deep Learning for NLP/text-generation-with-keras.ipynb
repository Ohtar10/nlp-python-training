{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM with Keras\n",
    "\n",
    "## Load the data\n",
    "We are going to use the moby dick text for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \"\"\"Read file.\n",
    "    Simple function to read all the text from a file.\n",
    "    Do not use it with large text files.\"\"\"\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call me Ishmael.  Some years ago--never mind how long\n",
      "precisely--having little or no money in my purse, and nothing\n",
      "particular to interest me on shore, I thought I would sail about a\n",
      "little and see the watery part of the world.  It is a way I have of\n",
      "driving off the spleen and regulating the circulation.  Whenever I\n",
      "find myself growing grim about the mouth; whenever it is a damp,\n",
      "drizzly November in my soul; whenever I find myself involuntarily\n",
      "pausing before coffin warehouses, and bringing up t ...\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../../datasets/moby_dick_four_chapters.txt\"\n",
    "corpus = read_file(file_path)\n",
    "print(corpus[:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import space disabling what we do not need.\n",
    "\n",
    "Remember that you need to download spacy data first:\n",
    "* https://spacy.io/\n",
    "* https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /home/ohtar10/miniconda3/envs/nlp-python-training/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/ohtar10/miniconda3/envs/nlp-python-training/lib/python3.7/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/ohtar10/miniconda3/envs/nlp-python-training/lib/python3.7/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
    "# This is needed in case we want to process a bigger text file\n",
    "nlp.max_length =1198623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the text a little bit by eliminating punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punctuation(doc_text, black_list='\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n '):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in black_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on',\n",
       " 'shore',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'i',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'about',\n",
       " 'a',\n",
       " 'little',\n",
       " 'and',\n",
       " 'see',\n",
       " 'the',\n",
       " 'watery',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'way',\n",
       " 'i',\n",
       " 'have',\n",
       " 'of',\n",
       " 'driving',\n",
       " 'off',\n",
       " 'the',\n",
       " 'spleen',\n",
       " 'and',\n",
       " 'regulating',\n",
       " 'the',\n",
       " 'circulation',\n",
       " 'whenever',\n",
       " 'i',\n",
       " 'find',\n",
       " 'myself',\n",
       " 'growing',\n",
       " 'grim',\n",
       " 'about',\n",
       " 'the',\n",
       " 'mouth',\n",
       " 'whenever',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'damp',\n",
       " 'drizzly',\n",
       " 'november',\n",
       " 'in',\n",
       " 'my',\n",
       " 'soul',\n",
       " 'whenever',\n",
       " 'i',\n",
       " 'find',\n",
       " 'myself',\n",
       " 'involuntarily',\n",
       " 'pausing',\n",
       " 'before',\n",
       " 'coffin',\n",
       " 'warehouses',\n",
       " 'and',\n",
       " 'bringing',\n",
       " 'up',\n",
       " 'the',\n",
       " 'rear',\n",
       " 'of',\n",
       " 'every',\n",
       " 'funeral',\n",
       " 'i',\n",
       " 'meet',\n",
       " 'and',\n",
       " 'especially',\n",
       " 'whenever',\n",
       " 'my',\n",
       " 'hypos',\n",
       " 'get',\n",
       " 'such',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'hand',\n",
       " 'of',\n",
       " 'me',\n",
       " 'that',\n",
       " 'it',\n",
       " 'requires',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'moral',\n",
       " 'principle',\n",
       " 'to',\n",
       " 'prevent',\n",
       " 'me',\n",
       " 'from',\n",
       " 'deliberately',\n",
       " 'stepping',\n",
       " 'into',\n",
       " 'the',\n",
       " 'street',\n",
       " 'and',\n",
       " 'methodically',\n",
       " 'knocking',\n",
       " 'people',\n",
       " \"'s\",\n",
       " 'hats',\n",
       " 'off',\n",
       " 'then',\n",
       " 'i',\n",
       " 'account',\n",
       " 'it',\n",
       " 'high',\n",
       " 'time',\n",
       " 'to',\n",
       " 'get',\n",
       " 'to',\n",
       " 'sea',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'i',\n",
       " 'can',\n",
       " 'this',\n",
       " 'is',\n",
       " 'my',\n",
       " 'substitute',\n",
       " 'for',\n",
       " 'pistol',\n",
       " 'and',\n",
       " 'ball',\n",
       " 'with',\n",
       " 'a',\n",
       " 'philosophical',\n",
       " 'flourish',\n",
       " 'cato',\n",
       " 'throws',\n",
       " 'himself',\n",
       " 'upon',\n",
       " 'his',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'quietly',\n",
       " 'take',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ship',\n",
       " 'there',\n",
       " 'is',\n",
       " 'nothing',\n",
       " 'surprising',\n",
       " 'in',\n",
       " 'this',\n",
       " 'if',\n",
       " 'they',\n",
       " 'but',\n",
       " 'knew',\n",
       " 'it',\n",
       " 'almost',\n",
       " 'all',\n",
       " 'men',\n",
       " 'in',\n",
       " 'their',\n",
       " 'degree',\n",
       " 'some',\n",
       " 'time',\n",
       " 'or',\n",
       " 'other',\n",
       " 'cherish',\n",
       " 'very',\n",
       " 'nearly',\n",
       " 'the',\n",
       " 'same',\n",
       " 'feelings',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'ocean',\n",
       " 'with',\n",
       " 'me',\n",
       " 'there',\n",
       " 'now',\n",
       " 'is',\n",
       " 'your',\n",
       " 'insular',\n",
       " 'city',\n",
       " 'of',\n",
       " 'the',\n",
       " 'manhattoes',\n",
       " 'belted',\n",
       " 'round',\n",
       " 'by',\n",
       " 'wharves',\n",
       " 'as',\n",
       " 'indian',\n",
       " 'isles',\n",
       " 'by',\n",
       " 'coral',\n",
       " 'reefs',\n",
       " 'commerce',\n",
       " 'surrounds',\n",
       " 'it',\n",
       " 'with',\n",
       " 'her',\n",
       " 'surf',\n",
       " 'right',\n",
       " 'and',\n",
       " 'left',\n",
       " 'the',\n",
       " 'streets',\n",
       " 'take',\n",
       " 'you',\n",
       " 'waterward',\n",
       " 'its',\n",
       " 'extreme',\n",
       " 'downtown',\n",
       " 'is',\n",
       " 'the',\n",
       " 'battery',\n",
       " 'where',\n",
       " 'that',\n",
       " 'noble',\n",
       " 'mole',\n",
       " 'is',\n",
       " 'washed',\n",
       " 'by',\n",
       " 'waves',\n",
       " 'and',\n",
       " 'cooled',\n",
       " 'by',\n",
       " 'breezes',\n",
       " 'which',\n",
       " 'a',\n",
       " 'few',\n",
       " 'hours',\n",
       " 'previous',\n",
       " 'were',\n",
       " 'out',\n",
       " 'of',\n",
       " 'sight',\n",
       " 'of',\n",
       " 'land',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'crowds',\n",
       " 'of',\n",
       " 'water',\n",
       " 'gazers',\n",
       " 'there',\n",
       " 'circumambulate',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'a',\n",
       " 'dreamy',\n",
       " 'sabbath',\n",
       " 'afternoon',\n",
       " 'go',\n",
       " 'from',\n",
       " 'corlears',\n",
       " 'hook',\n",
       " 'to',\n",
       " 'coenties',\n",
       " 'slip',\n",
       " 'and',\n",
       " 'from',\n",
       " 'thence',\n",
       " 'by',\n",
       " 'whitehall',\n",
       " 'northward',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'see',\n",
       " '?--',\n",
       " 'posted',\n",
       " 'like',\n",
       " 'silent',\n",
       " 'sentinels',\n",
       " 'all',\n",
       " 'around',\n",
       " 'the',\n",
       " 'town',\n",
       " 'stand',\n",
       " 'thousands',\n",
       " 'upon',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'mortal',\n",
       " 'men',\n",
       " 'fixed',\n",
       " 'in',\n",
       " 'ocean',\n",
       " 'reveries',\n",
       " 'some',\n",
       " 'leaning',\n",
       " 'against',\n",
       " 'the',\n",
       " 'spiles',\n",
       " 'some',\n",
       " 'seated',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'pier',\n",
       " 'heads',\n",
       " 'some',\n",
       " 'looking',\n",
       " 'over',\n",
       " 'the',\n",
       " 'bulwarks',\n",
       " 'of',\n",
       " 'ships',\n",
       " 'from',\n",
       " 'china',\n",
       " 'some',\n",
       " 'high',\n",
       " 'aloft',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rigging',\n",
       " 'as',\n",
       " 'if',\n",
       " 'striving',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'still',\n",
       " 'better',\n",
       " 'seaward',\n",
       " 'peep',\n",
       " 'but',\n",
       " 'these',\n",
       " 'are',\n",
       " 'all',\n",
       " 'landsmen',\n",
       " 'of',\n",
       " 'week',\n",
       " 'days',\n",
       " 'pent',\n",
       " 'up',\n",
       " 'in',\n",
       " 'lath',\n",
       " 'and',\n",
       " 'plaster',\n",
       " 'tied',\n",
       " 'to',\n",
       " 'counters',\n",
       " 'nailed',\n",
       " 'to',\n",
       " 'benches',\n",
       " 'clinched',\n",
       " 'to',\n",
       " 'desks',\n",
       " 'how',\n",
       " 'then',\n",
       " 'is',\n",
       " 'this',\n",
       " 'are',\n",
       " 'the',\n",
       " 'green',\n",
       " 'fields',\n",
       " 'gone',\n",
       " 'what',\n",
       " 'do',\n",
       " 'they',\n",
       " 'here',\n",
       " 'but',\n",
       " 'look',\n",
       " 'here',\n",
       " 'come',\n",
       " 'more',\n",
       " 'crowds',\n",
       " 'pacing',\n",
       " 'straight',\n",
       " 'for',\n",
       " 'the',\n",
       " 'water',\n",
       " 'and',\n",
       " 'seemingly',\n",
       " 'bound',\n",
       " 'for',\n",
       " 'a',\n",
       " 'dive',\n",
       " 'strange',\n",
       " 'nothing',\n",
       " 'will',\n",
       " 'content',\n",
       " 'them',\n",
       " 'but',\n",
       " 'the',\n",
       " 'extremest',\n",
       " 'limit',\n",
       " 'of',\n",
       " 'the',\n",
       " 'land',\n",
       " 'loitering',\n",
       " 'under',\n",
       " 'the',\n",
       " 'shady',\n",
       " 'lee',\n",
       " 'of',\n",
       " 'yonder',\n",
       " 'warehouses',\n",
       " 'will',\n",
       " 'not',\n",
       " 'suffice',\n",
       " 'no',\n",
       " 'they',\n",
       " 'must',\n",
       " 'get',\n",
       " 'just',\n",
       " 'as',\n",
       " 'nigh',\n",
       " 'the',\n",
       " 'water',\n",
       " 'as',\n",
       " 'they',\n",
       " 'possibly',\n",
       " 'can',\n",
       " 'without',\n",
       " 'falling',\n",
       " 'in',\n",
       " 'and',\n",
       " 'there',\n",
       " 'they',\n",
       " 'stand',\n",
       " 'miles',\n",
       " 'of',\n",
       " 'them',\n",
       " 'leagues',\n",
       " 'inlanders',\n",
       " 'all',\n",
       " 'they',\n",
       " 'come',\n",
       " 'from',\n",
       " 'lanes',\n",
       " 'and',\n",
       " 'alleys',\n",
       " 'streets',\n",
       " 'and',\n",
       " 'avenues',\n",
       " 'north',\n",
       " 'east',\n",
       " 'south',\n",
       " 'and',\n",
       " 'west',\n",
       " 'yet',\n",
       " 'here',\n",
       " 'they',\n",
       " 'all',\n",
       " 'unite',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'does',\n",
       " 'the',\n",
       " 'magnetic',\n",
       " 'virtue',\n",
       " 'of',\n",
       " 'the',\n",
       " 'needles',\n",
       " 'of',\n",
       " 'the',\n",
       " 'compasses',\n",
       " 'of',\n",
       " 'all',\n",
       " 'those',\n",
       " 'ships',\n",
       " 'attract',\n",
       " 'them',\n",
       " 'thither',\n",
       " 'once',\n",
       " 'more',\n",
       " 'say',\n",
       " 'you',\n",
       " 'are',\n",
       " 'in',\n",
       " 'the',\n",
       " 'country',\n",
       " 'in',\n",
       " 'some',\n",
       " 'high',\n",
       " 'land',\n",
       " 'of',\n",
       " 'lakes',\n",
       " 'take',\n",
       " 'almost',\n",
       " 'any',\n",
       " 'path',\n",
       " 'you',\n",
       " 'please',\n",
       " 'and',\n",
       " 'ten',\n",
       " 'to',\n",
       " 'one',\n",
       " 'it',\n",
       " 'carries',\n",
       " 'you',\n",
       " 'down',\n",
       " 'in',\n",
       " 'a',\n",
       " 'dale',\n",
       " 'and',\n",
       " 'leaves',\n",
       " 'you',\n",
       " 'there',\n",
       " 'by',\n",
       " 'a',\n",
       " 'pool',\n",
       " 'in',\n",
       " 'the',\n",
       " 'stream',\n",
       " 'there',\n",
       " 'is',\n",
       " 'magic',\n",
       " 'in',\n",
       " 'it',\n",
       " 'let',\n",
       " 'the',\n",
       " 'most',\n",
       " 'absent',\n",
       " 'minded',\n",
       " 'of',\n",
       " 'men',\n",
       " 'be',\n",
       " 'plunged',\n",
       " 'in',\n",
       " 'his',\n",
       " 'deepest',\n",
       " 'reveries',\n",
       " 'stand',\n",
       " 'that',\n",
       " 'man',\n",
       " 'on',\n",
       " 'his',\n",
       " 'legs',\n",
       " 'set',\n",
       " 'his',\n",
       " 'feet',\n",
       " 'a',\n",
       " 'going',\n",
       " 'and',\n",
       " 'he',\n",
       " 'will',\n",
       " 'infallibly',\n",
       " 'lead',\n",
       " 'you',\n",
       " 'to',\n",
       " 'water',\n",
       " 'if',\n",
       " 'water',\n",
       " 'there',\n",
       " 'be',\n",
       " 'in',\n",
       " 'all',\n",
       " 'that',\n",
       " 'region',\n",
       " 'should',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'be',\n",
       " 'athirst',\n",
       " 'in',\n",
       " 'the',\n",
       " 'great',\n",
       " 'american',\n",
       " 'desert',\n",
       " 'try',\n",
       " 'this',\n",
       " 'experiment',\n",
       " 'if',\n",
       " 'your',\n",
       " 'caravan',\n",
       " 'happen',\n",
       " 'to',\n",
       " 'be',\n",
       " 'supplied',\n",
       " 'with',\n",
       " 'a',\n",
       " 'metaphysical',\n",
       " 'professor',\n",
       " 'yes',\n",
       " 'as',\n",
       " 'every',\n",
       " 'one',\n",
       " 'knows',\n",
       " 'meditation',\n",
       " 'and',\n",
       " 'water',\n",
       " 'are',\n",
       " 'wedded',\n",
       " 'for',\n",
       " 'ever',\n",
       " 'but',\n",
       " 'here',\n",
       " 'is',\n",
       " 'an',\n",
       " 'artist',\n",
       " 'he',\n",
       " 'desires',\n",
       " 'to',\n",
       " 'paint',\n",
       " 'you',\n",
       " 'the',\n",
       " 'dreamiest',\n",
       " 'shadiest',\n",
       " 'quietest',\n",
       " 'most',\n",
       " 'enchanting',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'romantic',\n",
       " 'landscape',\n",
       " 'in',\n",
       " 'all',\n",
       " 'the',\n",
       " 'valley',\n",
       " 'of',\n",
       " 'the',\n",
       " 'saco',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'chief',\n",
       " 'element',\n",
       " 'he',\n",
       " 'employs',\n",
       " 'there',\n",
       " 'stand',\n",
       " 'his',\n",
       " 'trees',\n",
       " 'each',\n",
       " 'with',\n",
       " 'a',\n",
       " 'hollow',\n",
       " 'trunk',\n",
       " 'as',\n",
       " 'if',\n",
       " 'a',\n",
       " 'hermit',\n",
       " 'and',\n",
       " 'a',\n",
       " 'crucifix',\n",
       " 'were',\n",
       " 'within',\n",
       " 'and',\n",
       " 'here',\n",
       " 'sleeps',\n",
       " 'his',\n",
       " 'meadow',\n",
       " 'and',\n",
       " 'there',\n",
       " 'sleep',\n",
       " 'his',\n",
       " 'cattle',\n",
       " 'and',\n",
       " 'up',\n",
       " 'from',\n",
       " 'yonder',\n",
       " 'cottage',\n",
       " 'goes',\n",
       " 'a',\n",
       " 'sleepy',\n",
       " 'smoke',\n",
       " 'deep',\n",
       " 'into',\n",
       " 'distant',\n",
       " 'woodlands',\n",
       " 'winds',\n",
       " 'a',\n",
       " 'mazy',\n",
       " 'way',\n",
       " 'reaching',\n",
       " 'to',\n",
       " 'overlapping',\n",
       " 'spurs',\n",
       " 'of',\n",
       " 'mountains',\n",
       " 'bathed',\n",
       " 'in',\n",
       " 'their',\n",
       " 'hill',\n",
       " 'side',\n",
       " 'blue',\n",
       " 'but',\n",
       " 'though',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'lies',\n",
       " 'thus',\n",
       " 'tranced',\n",
       " 'and',\n",
       " 'though',\n",
       " 'this',\n",
       " 'pine',\n",
       " 'tree',\n",
       " 'shakes',\n",
       " 'down',\n",
       " 'its',\n",
       " 'sighs',\n",
       " 'like',\n",
       " 'leaves',\n",
       " 'upon',\n",
       " 'this',\n",
       " 'shepherd',\n",
       " \"'s\",\n",
       " 'head',\n",
       " 'yet',\n",
       " 'all',\n",
       " 'were',\n",
       " 'vain',\n",
       " 'unless',\n",
       " 'the',\n",
       " 'shepherd',\n",
       " \"'s\",\n",
       " 'eye',\n",
       " 'were',\n",
       " 'fixed',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'magic',\n",
       " 'stream',\n",
       " 'before',\n",
       " 'him',\n",
       " 'go',\n",
       " 'visit',\n",
       " 'the',\n",
       " 'prairies',\n",
       " 'in',\n",
       " 'june',\n",
       " 'when',\n",
       " 'for',\n",
       " 'scores',\n",
       " 'on',\n",
       " 'scores',\n",
       " 'of',\n",
       " 'miles',\n",
       " 'you',\n",
       " 'wade',\n",
       " 'knee',\n",
       " 'deep',\n",
       " 'among',\n",
       " 'tiger',\n",
       " 'lilies',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'one',\n",
       " 'charm',\n",
       " 'wanting',\n",
       " '?--',\n",
       " 'water',\n",
       " 'there',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'drop',\n",
       " 'of',\n",
       " 'water',\n",
       " 'there',\n",
       " 'were',\n",
       " 'niagara',\n",
       " 'but',\n",
       " 'a',\n",
       " 'cataract',\n",
       " 'of',\n",
       " 'sand',\n",
       " 'would',\n",
       " 'you',\n",
       " 'travel',\n",
       " 'your',\n",
       " 'thousand',\n",
       " 'miles',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'poor',\n",
       " 'poet',\n",
       " 'of',\n",
       " 'tennessee',\n",
       " 'upon',\n",
       " 'suddenly',\n",
       " 'receiving',\n",
       " 'two',\n",
       " 'handfuls',\n",
       " 'of',\n",
       " 'silver',\n",
       " 'deliberate',\n",
       " 'whether',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'him',\n",
       " 'a',\n",
       " 'coat',\n",
       " 'which',\n",
       " 'he',\n",
       " 'sadly',\n",
       " 'needed',\n",
       " 'or',\n",
       " 'invest',\n",
       " 'his',\n",
       " 'money',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pedestrian',\n",
       " 'trip',\n",
       " 'to',\n",
       " 'rockaway',\n",
       " 'beach',\n",
       " 'why',\n",
       " 'is',\n",
       " 'almost',\n",
       " 'every',\n",
       " 'robust',\n",
       " 'healthy',\n",
       " 'boy',\n",
       " 'with',\n",
       " 'a',\n",
       " 'robust',\n",
       " 'healthy',\n",
       " 'soul',\n",
       " 'in',\n",
       " 'him',\n",
       " 'at',\n",
       " 'some',\n",
       " 'time',\n",
       " 'or',\n",
       " 'other',\n",
       " 'crazy',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'sea',\n",
       " 'why',\n",
       " 'upon',\n",
       " 'your',\n",
       " 'first',\n",
       " 'voyage',\n",
       " 'as',\n",
       " 'a',\n",
       " 'passenger',\n",
       " 'did',\n",
       " 'you',\n",
       " 'yourself',\n",
       " 'feel',\n",
       " 'such',\n",
       " 'a',\n",
       " 'mystical',\n",
       " 'vibration',\n",
       " 'when',\n",
       " 'first',\n",
       " 'told',\n",
       " 'that',\n",
       " 'you',\n",
       " 'and',\n",
       " 'your',\n",
       " 'ship',\n",
       " 'were',\n",
       " 'now',\n",
       " 'out',\n",
       " 'of',\n",
       " 'sight',\n",
       " 'of',\n",
       " 'land',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'old',\n",
       " 'persians',\n",
       " 'hold',\n",
       " 'the',\n",
       " 'sea',\n",
       " 'holy',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'greeks',\n",
       " 'give',\n",
       " 'it',\n",
       " 'a',\n",
       " 'separate',\n",
       " 'deity',\n",
       " 'and',\n",
       " 'own',\n",
       " 'brother',\n",
       " 'of',\n",
       " 'jove',\n",
       " 'surely',\n",
       " 'all',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'without',\n",
       " 'meaning',\n",
       " 'and',\n",
       " 'still',\n",
       " 'deeper',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'that',\n",
       " 'story',\n",
       " 'of',\n",
       " 'narcissus',\n",
       " 'who',\n",
       " 'because',\n",
       " 'he',\n",
       " 'could',\n",
       " 'not',\n",
       " 'grasp',\n",
       " 'the',\n",
       " 'tormenting',\n",
       " 'mild',\n",
       " 'image',\n",
       " 'he',\n",
       " 'saw',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fountain',\n",
       " 'plunged',\n",
       " 'into',\n",
       " 'it',\n",
       " 'and',\n",
       " 'was',\n",
       " 'drowned',\n",
       " 'but',\n",
       " 'that',\n",
       " 'same',\n",
       " 'image',\n",
       " 'we',\n",
       " 'ourselves',\n",
       " 'see',\n",
       " 'in',\n",
       " 'all',\n",
       " 'rivers',\n",
       " 'and',\n",
       " 'oceans',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'image',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ungraspable',\n",
       " 'phantom',\n",
       " 'of',\n",
       " 'life',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'key',\n",
       " 'to',\n",
       " 'it',\n",
       " 'all',\n",
       " 'now',\n",
       " 'when',\n",
       " 'i',\n",
       " 'say',\n",
       " 'that',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'the',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'going',\n",
       " 'to',\n",
       " 'sea',\n",
       " 'whenever',\n",
       " 'i',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'grow',\n",
       " 'hazy',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = separate_punctuation(corpus)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the next word\n",
    "First, we want to predict words given a certain sequence of previous words, say given the first 25 tokens, we will try to predict the number 26. For this, we need to create token sequencies to feed the neural network.\n",
    "\n",
    "The number of words for the sequence may vary depending on the use case. This needs to be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_sequences(train_len, tokens):\n",
    "    text_sequences = []\n",
    "    for i in range(train_len, len(tokens)):\n",
    "        # basically go train_len characters back \n",
    "        seq = tokens[i - train_len: i]\n",
    "        text_sequences.append(seq)\n",
    "    return text_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['call',\n",
       "  'me',\n",
       "  'ishmael',\n",
       "  'some',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'never',\n",
       "  'mind',\n",
       "  'how',\n",
       "  'long',\n",
       "  'precisely',\n",
       "  'having',\n",
       "  'little',\n",
       "  'or',\n",
       "  'no',\n",
       "  'money',\n",
       "  'in',\n",
       "  'my',\n",
       "  'purse',\n",
       "  'and',\n",
       "  'nothing',\n",
       "  'particular',\n",
       "  'to',\n",
       "  'interest',\n",
       "  'me',\n",
       "  'on'],\n",
       " ['me',\n",
       "  'ishmael',\n",
       "  'some',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'never',\n",
       "  'mind',\n",
       "  'how',\n",
       "  'long',\n",
       "  'precisely',\n",
       "  'having',\n",
       "  'little',\n",
       "  'or',\n",
       "  'no',\n",
       "  'money',\n",
       "  'in',\n",
       "  'my',\n",
       "  'purse',\n",
       "  'and',\n",
       "  'nothing',\n",
       "  'particular',\n",
       "  'to',\n",
       "  'interest',\n",
       "  'me',\n",
       "  'on',\n",
       "  'shore'],\n",
       " ['ishmael',\n",
       "  'some',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'never',\n",
       "  'mind',\n",
       "  'how',\n",
       "  'long',\n",
       "  'precisely',\n",
       "  'having',\n",
       "  'little',\n",
       "  'or',\n",
       "  'no',\n",
       "  'money',\n",
       "  'in',\n",
       "  'my',\n",
       "  'purse',\n",
       "  'and',\n",
       "  'nothing',\n",
       "  'particular',\n",
       "  'to',\n",
       "  'interest',\n",
       "  'me',\n",
       "  'on',\n",
       "  'shore',\n",
       "  'i'],\n",
       " ['some',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'never',\n",
       "  'mind',\n",
       "  'how',\n",
       "  'long',\n",
       "  'precisely',\n",
       "  'having',\n",
       "  'little',\n",
       "  'or',\n",
       "  'no',\n",
       "  'money',\n",
       "  'in',\n",
       "  'my',\n",
       "  'purse',\n",
       "  'and',\n",
       "  'nothing',\n",
       "  'particular',\n",
       "  'to',\n",
       "  'interest',\n",
       "  'me',\n",
       "  'on',\n",
       "  'shore',\n",
       "  'i',\n",
       "  'thought'],\n",
       " ['years',\n",
       "  'ago',\n",
       "  'never',\n",
       "  'mind',\n",
       "  'how',\n",
       "  'long',\n",
       "  'precisely',\n",
       "  'having',\n",
       "  'little',\n",
       "  'or',\n",
       "  'no',\n",
       "  'money',\n",
       "  'in',\n",
       "  'my',\n",
       "  'purse',\n",
       "  'and',\n",
       "  'nothing',\n",
       "  'particular',\n",
       "  'to',\n",
       "  'interest',\n",
       "  'me',\n",
       "  'on',\n",
       "  'shore',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'i']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences = create_token_sequences(26, tokens)\n",
    "text_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the result is like a sliding window of one word over the text, each sequence just moves one word to the right every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with keras.\n",
    "\n",
    "First, as we know, neural networks do not work with text but numbers, hence we need to convert the text sequences into numeric sequences. For this purpose, keras has a built-in Tokenizer that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[964,\n",
       "  14,\n",
       "  265,\n",
       "  51,\n",
       "  263,\n",
       "  416,\n",
       "  87,\n",
       "  222,\n",
       "  129,\n",
       "  111,\n",
       "  962,\n",
       "  262,\n",
       "  50,\n",
       "  43,\n",
       "  37,\n",
       "  321,\n",
       "  7,\n",
       "  23,\n",
       "  555,\n",
       "  3,\n",
       "  150,\n",
       "  261,\n",
       "  6,\n",
       "  2704,\n",
       "  14,\n",
       "  24],\n",
       " [14,\n",
       "  265,\n",
       "  51,\n",
       "  263,\n",
       "  416,\n",
       "  87,\n",
       "  222,\n",
       "  129,\n",
       "  111,\n",
       "  962,\n",
       "  262,\n",
       "  50,\n",
       "  43,\n",
       "  37,\n",
       "  321,\n",
       "  7,\n",
       "  23,\n",
       "  555,\n",
       "  3,\n",
       "  150,\n",
       "  261,\n",
       "  6,\n",
       "  2704,\n",
       "  14,\n",
       "  24,\n",
       "  965],\n",
       " [265,\n",
       "  51,\n",
       "  263,\n",
       "  416,\n",
       "  87,\n",
       "  222,\n",
       "  129,\n",
       "  111,\n",
       "  962,\n",
       "  262,\n",
       "  50,\n",
       "  43,\n",
       "  37,\n",
       "  321,\n",
       "  7,\n",
       "  23,\n",
       "  555,\n",
       "  3,\n",
       "  150,\n",
       "  261,\n",
       "  6,\n",
       "  2704,\n",
       "  14,\n",
       "  24,\n",
       "  965,\n",
       "  5],\n",
       " [51,\n",
       "  263,\n",
       "  416,\n",
       "  87,\n",
       "  222,\n",
       "  129,\n",
       "  111,\n",
       "  962,\n",
       "  262,\n",
       "  50,\n",
       "  43,\n",
       "  37,\n",
       "  321,\n",
       "  7,\n",
       "  23,\n",
       "  555,\n",
       "  3,\n",
       "  150,\n",
       "  261,\n",
       "  6,\n",
       "  2704,\n",
       "  14,\n",
       "  24,\n",
       "  965,\n",
       "  5,\n",
       "  60],\n",
       " [263,\n",
       "  416,\n",
       "  87,\n",
       "  222,\n",
       "  129,\n",
       "  111,\n",
       "  962,\n",
       "  262,\n",
       "  50,\n",
       "  43,\n",
       "  37,\n",
       "  321,\n",
       "  7,\n",
       "  23,\n",
       "  555,\n",
       "  3,\n",
       "  150,\n",
       "  261,\n",
       "  6,\n",
       "  2704,\n",
       "  14,\n",
       "  24,\n",
       "  965,\n",
       "  5,\n",
       "  60,\n",
       "  5]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we basically obtained is the same word sequences but each word is encoded with a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'a',\n",
       " 3: 'and',\n",
       " 4: 'of',\n",
       " 5: 'i',\n",
       " 6: 'to',\n",
       " 7: 'in',\n",
       " 8: 'it',\n",
       " 9: 'that',\n",
       " 10: 'he',\n",
       " 11: 'his',\n",
       " 12: 'was',\n",
       " 13: 'but',\n",
       " 14: 'me',\n",
       " 15: 'with',\n",
       " 16: 'as',\n",
       " 17: 'you',\n",
       " 18: 'this',\n",
       " 19: 'at',\n",
       " 20: 'is',\n",
       " 21: 'all',\n",
       " 22: 'for',\n",
       " 23: 'my',\n",
       " 24: 'on',\n",
       " 25: 'be',\n",
       " 26: \"'s\",\n",
       " 27: 'not',\n",
       " 28: 'from',\n",
       " 29: 'there',\n",
       " 30: 'one',\n",
       " 31: 'up',\n",
       " 32: 'what',\n",
       " 33: 'him',\n",
       " 34: 'so',\n",
       " 35: 'bed',\n",
       " 36: 'now',\n",
       " 37: 'no',\n",
       " 38: 'about',\n",
       " 39: 'into',\n",
       " 40: 'by',\n",
       " 41: 'were',\n",
       " 42: 'out',\n",
       " 43: 'or',\n",
       " 44: 'harpooneer',\n",
       " 45: 'had',\n",
       " 46: 'then',\n",
       " 47: 'have',\n",
       " 48: 'an',\n",
       " 49: 'upon',\n",
       " 50: 'little',\n",
       " 51: 'some',\n",
       " 52: 'old',\n",
       " 53: 'like',\n",
       " 54: 'if',\n",
       " 55: 'they',\n",
       " 56: 'would',\n",
       " 57: 'do',\n",
       " 58: 'over',\n",
       " 59: 'landlord',\n",
       " 60: 'thought',\n",
       " 61: 'room',\n",
       " 62: 'when',\n",
       " 63: 'could',\n",
       " 64: 'here',\n",
       " 65: 'head',\n",
       " 66: \"n't\",\n",
       " 67: 'night',\n",
       " 68: 'such',\n",
       " 69: 'which',\n",
       " 70: 'man',\n",
       " 71: 'did',\n",
       " 72: 'sea',\n",
       " 73: 'though',\n",
       " 74: 'time',\n",
       " 75: 'other',\n",
       " 76: 'very',\n",
       " 77: 'go',\n",
       " 78: 'these',\n",
       " 79: 'more',\n",
       " 80: 'first',\n",
       " 81: 'sort',\n",
       " 82: 'said',\n",
       " 83: 'last',\n",
       " 84: 'down',\n",
       " 85: 'most',\n",
       " 86: 'been',\n",
       " 87: 'never',\n",
       " 88: 'see',\n",
       " 89: 'your',\n",
       " 90: 'them',\n",
       " 91: 'must',\n",
       " 92: 'tell',\n",
       " 93: 'much',\n",
       " 94: 'good',\n",
       " 95: 'off',\n",
       " 96: 'myself',\n",
       " 97: 'are',\n",
       " 98: 'yet',\n",
       " 99: 'sleep',\n",
       " 100: 'who',\n",
       " 101: 'seemed',\n",
       " 102: 'light',\n",
       " 103: 'way',\n",
       " 104: 'their',\n",
       " 105: 'just',\n",
       " 106: 'being',\n",
       " 107: 'than',\n",
       " 108: 'place',\n",
       " 109: 'queequeg',\n",
       " 110: 'great',\n",
       " 111: 'long',\n",
       " 112: 'before',\n",
       " 113: 'get',\n",
       " 114: 'round',\n",
       " 115: 'where',\n",
       " 116: 'still',\n",
       " 117: 'any',\n",
       " 118: 'too',\n",
       " 119: 'only',\n",
       " 120: 'door',\n",
       " 121: 'can',\n",
       " 122: 'himself',\n",
       " 123: 'heads',\n",
       " 124: 'come',\n",
       " 125: 'ever',\n",
       " 126: 'two',\n",
       " 127: 'enough',\n",
       " 128: 'made',\n",
       " 129: 'how',\n",
       " 130: 'hand',\n",
       " 131: 'same',\n",
       " 132: 'looking',\n",
       " 133: 'something',\n",
       " 134: 'may',\n",
       " 135: \"'\",\n",
       " 136: 'almost',\n",
       " 137: 'say',\n",
       " 138: 'should',\n",
       " 139: 'side',\n",
       " 140: 'why',\n",
       " 141: 'own',\n",
       " 142: 'we',\n",
       " 143: 'new',\n",
       " 144: 'again',\n",
       " 145: 'came',\n",
       " 146: 'arm',\n",
       " 147: 'house',\n",
       " 148: 'away',\n",
       " 149: 'might',\n",
       " 150: 'nothing',\n",
       " 151: 'take',\n",
       " 152: 'towards',\n",
       " 153: 'water',\n",
       " 154: 'will',\n",
       " 155: 'under',\n",
       " 156: 'going',\n",
       " 157: 'rather',\n",
       " 158: 'make',\n",
       " 159: 'whale',\n",
       " 160: 'stood',\n",
       " 161: 'boots',\n",
       " 162: 'ye',\n",
       " 163: 'back',\n",
       " 164: \"'ll\",\n",
       " 165: 'tomahawk',\n",
       " 166: 'part',\n",
       " 167: 'world',\n",
       " 168: 'soon',\n",
       " 169: 'against',\n",
       " 170: 'those',\n",
       " 171: 'between',\n",
       " 172: 'after',\n",
       " 173: 'whaling',\n",
       " 174: 'lay',\n",
       " 175: 'took',\n",
       " 176: 'half',\n",
       " 177: 'began',\n",
       " 178: 'face',\n",
       " 179: 'streets',\n",
       " 180: 'land',\n",
       " 181: 'better',\n",
       " 182: 'once',\n",
       " 183: 'voyage',\n",
       " 184: 'give',\n",
       " 185: 'well',\n",
       " 186: 'however',\n",
       " 187: 'else',\n",
       " 188: 'heard',\n",
       " 189: 'put',\n",
       " 190: 'stop',\n",
       " 191: 'dark',\n",
       " 192: 'went',\n",
       " 193: 'black',\n",
       " 194: 'window',\n",
       " 195: 'cannibal',\n",
       " 196: 'fire',\n",
       " 197: 'every',\n",
       " 198: 'ship',\n",
       " 199: '?--',\n",
       " 200: 'town',\n",
       " 201: 'stand',\n",
       " 202: 'strange',\n",
       " 203: 'without',\n",
       " 204: 'feet',\n",
       " 205: 'whether',\n",
       " 206: 'because',\n",
       " 207: 'eyes',\n",
       " 208: 'think',\n",
       " 209: 'thinks',\n",
       " 210: 'idea',\n",
       " 211: 'bag',\n",
       " 212: 'nantucket',\n",
       " 213: 'late',\n",
       " 214: 'cold',\n",
       " 215: 'our',\n",
       " 216: 'found',\n",
       " 217: 'full',\n",
       " 218: 'morning',\n",
       " 219: 'sleeping',\n",
       " 220: 'feeling',\n",
       " 221: 'got',\n",
       " 222: 'mind',\n",
       " 223: 'her',\n",
       " 224: 'right',\n",
       " 225: 'its',\n",
       " 226: 'look',\n",
       " 227: 'south',\n",
       " 228: 'does',\n",
       " 229: 'let',\n",
       " 230: 'set',\n",
       " 231: 'yourself',\n",
       " 232: 'image',\n",
       " 233: 'saw',\n",
       " 234: 'am',\n",
       " 235: 'besides',\n",
       " 236: 'sailor',\n",
       " 237: 'seas',\n",
       " 238: 'rolled',\n",
       " 239: 'till',\n",
       " 240: 'day',\n",
       " 241: 'sign',\n",
       " 242: 'looked',\n",
       " 243: 'hard',\n",
       " 244: 'moment',\n",
       " 245: 'corner',\n",
       " 246: 'entry',\n",
       " 247: '.--',\n",
       " 248: 'four',\n",
       " 249: 'wall',\n",
       " 250: 'savage',\n",
       " 251: 'table',\n",
       " 252: 'indeed',\n",
       " 253: 'bench',\n",
       " 254: 'chest',\n",
       " 255: 'while',\n",
       " 256: 'stranger',\n",
       " 257: 'possible',\n",
       " 258: 'floor',\n",
       " 259: 'squares',\n",
       " 260: 'hat',\n",
       " 261: 'particular',\n",
       " 262: 'having',\n",
       " 263: 'years',\n",
       " 264: 'harpoon',\n",
       " 265: 'ishmael',\n",
       " 266: 'whenever',\n",
       " 267: 'mouth',\n",
       " 268: 'coffin',\n",
       " 269: 'high',\n",
       " 270: 'knew',\n",
       " 271: 'men',\n",
       " 272: 'hours',\n",
       " 273: 'green',\n",
       " 274: 'bit',\n",
       " 275: 'within',\n",
       " 276: 'picture',\n",
       " 277: 'told',\n",
       " 278: 'story',\n",
       " 279: 'mean',\n",
       " 280: 'thing',\n",
       " 281: ',--',\n",
       " 282: 'speak',\n",
       " 283: 'order',\n",
       " 284: 'making',\n",
       " 285: 'even',\n",
       " 286: 'perhaps',\n",
       " 287: 'things',\n",
       " 288: 'answer',\n",
       " 289: 'parts',\n",
       " 290: 'wild',\n",
       " 291: 'reason',\n",
       " 292: 'young',\n",
       " 293: 'craft',\n",
       " 294: 'business',\n",
       " 295: 'dead',\n",
       " 296: 'another',\n",
       " 297: 'middle',\n",
       " 298: 'sure',\n",
       " 299: 'candle',\n",
       " 300: 'presently',\n",
       " 301: 'low',\n",
       " 302: 'turned',\n",
       " 303: 'teeth',\n",
       " 304: 'dim',\n",
       " 305: 'euroclydon',\n",
       " 306: 'kept',\n",
       " 307: 'glass',\n",
       " 308: 'afterwards',\n",
       " 309: 'large',\n",
       " 310: 'three',\n",
       " 311: 'through',\n",
       " 312: 'telling',\n",
       " 313: 'getting',\n",
       " 314: 'small',\n",
       " 315: 'next',\n",
       " 316: 'seeing',\n",
       " 317: 'sell',\n",
       " 318: 'felt',\n",
       " 319: 'sun',\n",
       " 320: 'e',\n",
       " 321: 'money',\n",
       " 322: 'sail',\n",
       " 323: 'especially',\n",
       " 324: 'street',\n",
       " 325: 'city',\n",
       " 326: 'few',\n",
       " 327: 'previous',\n",
       " 328: 'sight',\n",
       " 329: 'days',\n",
       " 330: 'straight',\n",
       " 331: 'nigh',\n",
       " 332: 'legs',\n",
       " 333: 'try',\n",
       " 334: 'yes',\n",
       " 335: 'unless',\n",
       " 336: 'poor',\n",
       " 337: 'coat',\n",
       " 338: 'passenger',\n",
       " 339: ';--',\n",
       " 340: 'taking',\n",
       " 341: 'true',\n",
       " 342: 'ai',\n",
       " 343: 'always',\n",
       " 344: 'us',\n",
       " 345: 'really',\n",
       " 346: 'marvellous',\n",
       " 347: 'heaven',\n",
       " 348: 'air',\n",
       " 349: 'far',\n",
       " 350: 'second',\n",
       " 351: 'many',\n",
       " 352: 'has',\n",
       " 353: 'unaccountable',\n",
       " 354: 'grand',\n",
       " 355: 'jolly',\n",
       " 356: 'open',\n",
       " 357: 'shirt',\n",
       " 358: 'cape',\n",
       " 359: 'bedford',\n",
       " 360: 'fine',\n",
       " 361: 'further',\n",
       " 362: 'fish',\n",
       " 363: 'inn',\n",
       " 364: 'ice',\n",
       " 365: 'frost',\n",
       " 366: 'foot',\n",
       " 367: 'wide',\n",
       " 368: 'white',\n",
       " 369: 'tall',\n",
       " 370: 'spouter',\n",
       " 371: 'i.',\n",
       " 372: 'wooden',\n",
       " 373: 'worse',\n",
       " 374: 'death',\n",
       " 375: 'mine',\n",
       " 376: 'lazarus',\n",
       " 377: 'keep',\n",
       " 378: 'along',\n",
       " 379: 'hung',\n",
       " 380: 'throwing',\n",
       " 381: 'centre',\n",
       " 382: 'rest',\n",
       " 383: 'fact',\n",
       " 384: 'hair',\n",
       " 385: 'broken',\n",
       " 386: 'kill',\n",
       " 387: 'chimney',\n",
       " 388: 'fancy',\n",
       " 389: 'bar',\n",
       " 390: 'supper',\n",
       " 391: 'trying',\n",
       " 392: 'dumplings',\n",
       " 393: 'heavens',\n",
       " 394: 'manner',\n",
       " 395: 'devil',\n",
       " 396: 'together',\n",
       " 397: 'seen',\n",
       " 398: 'deal',\n",
       " 399: 'know',\n",
       " 400: 'skin',\n",
       " 401: 'ca',\n",
       " 402: 'shavings',\n",
       " 403: 'peddling',\n",
       " 404: 'sunday',\n",
       " 405: 'counterpane',\n",
       " 406: 'mat',\n",
       " 407: 'christian',\n",
       " 408: 'commenced',\n",
       " 409: 'thinking',\n",
       " 410: 'similar',\n",
       " 411: 'afraid',\n",
       " 412: 'length',\n",
       " 413: 'idol',\n",
       " 414: 'sabbee',\n",
       " 415: 'waking',\n",
       " 416: 'ago',\n",
       " 417: 'find',\n",
       " 418: 'damp',\n",
       " 419: 'soul',\n",
       " 420: 'strong',\n",
       " 421: 'account',\n",
       " 422: 'sword',\n",
       " 423: 'quietly',\n",
       " 424: 'degree',\n",
       " 425: 'left',\n",
       " 426: 'around',\n",
       " 427: 'fixed',\n",
       " 428: 'ships',\n",
       " 429: 'miles',\n",
       " 430: 'country',\n",
       " 431: 'stream',\n",
       " 432: 'lead',\n",
       " 433: 'american',\n",
       " 434: 'artist',\n",
       " 435: 'each',\n",
       " 436: 'goes',\n",
       " 437: 'deep',\n",
       " 438: 'distant',\n",
       " 439: 'winds',\n",
       " 440: 'blue',\n",
       " 441: 'among',\n",
       " 442: 'suddenly',\n",
       " 443: 'feel',\n",
       " 444: 'meaning',\n",
       " 445: 'phantom',\n",
       " 446: 'life',\n",
       " 447: 'passengers',\n",
       " 448: 'nor',\n",
       " 449: 'cook',\n",
       " 450: 'kind',\n",
       " 451: 'quite',\n",
       " 452: 'care',\n",
       " 453: 'board',\n",
       " 454: 'somehow',\n",
       " 455: 'broiled',\n",
       " 456: 'mast',\n",
       " 457: 'sense',\n",
       " 458: 'knowing',\n",
       " 459: 'either',\n",
       " 460: 'passed',\n",
       " 461: 'hands',\n",
       " 462: 'paying',\n",
       " 463: 'pay',\n",
       " 464: 'penny',\n",
       " 465: 'sailors',\n",
       " 466: 'exactly',\n",
       " 467: 'short',\n",
       " 468: 'easy',\n",
       " 469: 'portentous',\n",
       " 470: 'island',\n",
       " 471: 'nameless',\n",
       " 472: 'sounds',\n",
       " 473: 'since',\n",
       " 474: 'snow',\n",
       " 475: 'saturday',\n",
       " 476: 'matter',\n",
       " 477: 'red',\n",
       " 478: 'partly',\n",
       " 479: 'ere',\n",
       " 480: 'became',\n",
       " 481: 'meanwhile',\n",
       " 482: 'pocket',\n",
       " 483: 'darkness',\n",
       " 484: 'harpoons',\n",
       " 485: '\"--',\n",
       " 486: 'watch',\n",
       " 487: 'broad',\n",
       " 488: 'entering',\n",
       " 489: 'ha',\n",
       " 490: 'ashes',\n",
       " 491: 'opened',\n",
       " 492: 'peter',\n",
       " 493: 'name',\n",
       " 494: 'suppose',\n",
       " 495: 'quiet',\n",
       " 496: 'best',\n",
       " 497: 'tempestuous',\n",
       " 498: 'says',\n",
       " 499: 'thou',\n",
       " 500: 'both',\n",
       " 501: 'occurred',\n",
       " 502: 'dives',\n",
       " 503: 'holding',\n",
       " 504: 'frozen',\n",
       " 505: 'plenty',\n",
       " 506: 'altogether',\n",
       " 507: 'plain',\n",
       " 508: 'whom',\n",
       " 509: 'clean',\n",
       " 510: 'human',\n",
       " 511: 'entered',\n",
       " 512: 'wrinkled',\n",
       " 513: 'shelf',\n",
       " 514: 'jonah',\n",
       " 515: 'blanket',\n",
       " 516: \"goin'\",\n",
       " 517: 'bitter',\n",
       " 518: 'sat',\n",
       " 519: 'settle',\n",
       " 520: 'chap',\n",
       " 521: 'help',\n",
       " 522: 'spend',\n",
       " 523: 'landed',\n",
       " 524: 'standing',\n",
       " 525: 'held',\n",
       " 526: 'somewhat',\n",
       " 527: 'sober',\n",
       " 528: 'whole',\n",
       " 529: 'dam',\n",
       " 530: 'brown',\n",
       " 531: 'bulkington',\n",
       " 532: \"o'clock\",\n",
       " 533: 'none',\n",
       " 534: 'coming',\n",
       " 535: \"'ve\",\n",
       " 536: 'wait',\n",
       " 537: 'plane',\n",
       " 538: 'saying',\n",
       " 539: 'grinning',\n",
       " 540: 'placed',\n",
       " 541: 'shouted',\n",
       " 542: 'bedfellow',\n",
       " 543: 'zealand',\n",
       " 544: 'sal',\n",
       " 545: 'wash',\n",
       " 546: 'thrown',\n",
       " 547: 'purplish',\n",
       " 548: 'turn',\n",
       " 549: 'completely',\n",
       " 550: 'fear',\n",
       " 551: 'grego',\n",
       " 552: 'baby',\n",
       " 553: 'slowly',\n",
       " 554: 'civilized',\n",
       " 555: 'purse',\n",
       " 556: 'monkey',\n",
       " 557: 'involuntarily',\n",
       " 558: 'pausing',\n",
       " 559: 'warehouses',\n",
       " 560: 'requires',\n",
       " 561: 'people',\n",
       " 562: 'nearly',\n",
       " 563: 'ocean',\n",
       " 564: 'indian',\n",
       " 565: 'waterward',\n",
       " 566: 'battery',\n",
       " 567: 'noble',\n",
       " 568: 'washed',\n",
       " 569: 'crowds',\n",
       " 570: 'sabbath',\n",
       " 571: 'afternoon',\n",
       " 572: 'thence',\n",
       " 573: 'silent',\n",
       " 574: 'thousands',\n",
       " 575: 'reveries',\n",
       " 576: 'leaning',\n",
       " 577: 'seated',\n",
       " 578: 'bulwarks',\n",
       " 579: 'aloft',\n",
       " 580: 'week',\n",
       " 581: 'plaster',\n",
       " 582: 'gone',\n",
       " 583: 'bound',\n",
       " 584: 'content',\n",
       " 585: 'yonder',\n",
       " 586: 'falling',\n",
       " 587: 'north',\n",
       " 588: 'please',\n",
       " 589: 'ten',\n",
       " 590: 'leaves',\n",
       " 591: 'magic',\n",
       " 592: 'plunged',\n",
       " 593: 'metaphysical',\n",
       " 594: 'chief',\n",
       " 595: 'trunk',\n",
       " 596: 'meadow',\n",
       " 597: 'smoke',\n",
       " 598: 'reaching',\n",
       " 599: 'hill',\n",
       " 600: 'thus',\n",
       " 601: 'pine',\n",
       " 602: 'sighs',\n",
       " 603: 'shepherd',\n",
       " 604: 'june',\n",
       " 605: 'scores',\n",
       " 606: 'thousand',\n",
       " 607: 'silver',\n",
       " 608: 'sadly',\n",
       " 609: 'robust',\n",
       " 610: 'healthy',\n",
       " 611: 'boy',\n",
       " 612: 'crazy',\n",
       " 613: 'hold',\n",
       " 614: 'holy',\n",
       " 615: 'brother',\n",
       " 616: 'ourselves',\n",
       " 617: 'begin',\n",
       " 618: 'grow',\n",
       " 619: 'inferred',\n",
       " 620: 'needs',\n",
       " 621: \"don't\",\n",
       " 622: 'themselves',\n",
       " 623: 'commodore',\n",
       " 624: 'captain',\n",
       " 625: 'glory',\n",
       " 626: 'whatsoever',\n",
       " 627: 'confess',\n",
       " 628: 'officer',\n",
       " 629: 'respectfully',\n",
       " 630: 'horse',\n",
       " 631: 'huge',\n",
       " 632: 'houses',\n",
       " 633: 'forecastle',\n",
       " 634: 'jump',\n",
       " 635: 'spar',\n",
       " 636: 'particularly',\n",
       " 637: 'putting',\n",
       " 638: 'tar',\n",
       " 639: 'schoolmaster',\n",
       " 640: 'boys',\n",
       " 641: 'transition',\n",
       " 642: 'grin',\n",
       " 643: 'bear',\n",
       " 644: 'hunks',\n",
       " 645: 'sweep',\n",
       " 646: 'weighed',\n",
       " 647: 'anything',\n",
       " 648: 'less',\n",
       " 649: 'thump',\n",
       " 650: 'point',\n",
       " 651: 'view',\n",
       " 652: 'single',\n",
       " 653: 'difference',\n",
       " 654: 'paid',\n",
       " 655: 'act',\n",
       " 656: 'uncomfortable',\n",
       " 657: 'compare',\n",
       " 658: 'earthly',\n",
       " 659: 'enter',\n",
       " 660: 'deck',\n",
       " 661: 'quarter',\n",
       " 662: 'leaders',\n",
       " 663: 'smelt',\n",
       " 664: 'fates',\n",
       " 665: 'doubtless',\n",
       " 666: 'formed',\n",
       " 667: 'run',\n",
       " 668: 'stage',\n",
       " 669: 'shabby',\n",
       " 670: 'others',\n",
       " 671: 'circumstances',\n",
       " 672: 'motives',\n",
       " 673: 'various',\n",
       " 674: 'mysterious',\n",
       " 675: 'curiosity',\n",
       " 676: 'tormented',\n",
       " 677: 'everlasting',\n",
       " 678: 'wonder',\n",
       " 679: 'purpose',\n",
       " 680: 'floated',\n",
       " 681: 'stuffed',\n",
       " 682: 'horn',\n",
       " 683: 'arrived',\n",
       " 684: 'offer',\n",
       " 685: 'following',\n",
       " 686: 'embark',\n",
       " 687: 'pleased',\n",
       " 688: 'original',\n",
       " 689: 'stranded',\n",
       " 690: 'leviathan',\n",
       " 691: 'whales',\n",
       " 692: 'nay',\n",
       " 693: 'dismal',\n",
       " 694: 'wherever',\n",
       " 695: 'dreary',\n",
       " 696: 'crossed',\n",
       " 697: 'expensive',\n",
       " 698: 'bright',\n",
       " 699: 'windows',\n",
       " 700: 'packed',\n",
       " 701: 'inches',\n",
       " 702: 'thick',\n",
       " 703: 'hear',\n",
       " 704: 'tinkling',\n",
       " 705: 'glasses',\n",
       " 706: 'blackness',\n",
       " 707: 'moving',\n",
       " 708: 'hour',\n",
       " 709: 'proved',\n",
       " 710: 'meant',\n",
       " 711: 'public',\n",
       " 712: 'box',\n",
       " 713: 'flying',\n",
       " 714: '?\"--',\n",
       " 715: 'trap',\n",
       " 716: 'loud',\n",
       " 717: 'voice',\n",
       " 718: 'sitting',\n",
       " 719: 'beyond',\n",
       " 720: 'negro',\n",
       " 721: 'creaking',\n",
       " 722: 'swinging',\n",
       " 723: 'painting',\n",
       " 724: 'representing',\n",
       " 725: 'connexion',\n",
       " 726: 'itself',\n",
       " 727: 'carted',\n",
       " 728: 'burnt',\n",
       " 729: 'spot',\n",
       " 730: 'queer',\n",
       " 731: 'gable',\n",
       " 732: 'ended',\n",
       " 733: 'sharp',\n",
       " 734: 'wind',\n",
       " 735: 'howling',\n",
       " 736: 'nevertheless',\n",
       " 737: 'called',\n",
       " 738: 'outside',\n",
       " 739: 'passage',\n",
       " 740: 'body',\n",
       " 741: 'curbstone',\n",
       " 742: 'corn',\n",
       " 743: 'pooh',\n",
       " 744: 'northern',\n",
       " 745: 'lights',\n",
       " 746: 'summer',\n",
       " 747: 'lengthwise',\n",
       " 748: 'gods',\n",
       " 749: 'lie',\n",
       " 750: 'study',\n",
       " 751: 'series',\n",
       " 752: 'arrive',\n",
       " 753: 'shadows',\n",
       " 754: 'dint',\n",
       " 755: 'conclusion',\n",
       " 756: 'confounded',\n",
       " 757: 'limber',\n",
       " 758: 'truly',\n",
       " 759: 'drive',\n",
       " 760: 'unimaginable',\n",
       " 761: 'midnight',\n",
       " 762: 'unnatural',\n",
       " 763: 'breaking',\n",
       " 764: 'design',\n",
       " 765: 'opposite',\n",
       " 766: 'monstrous',\n",
       " 767: 'knots',\n",
       " 768: 'vast',\n",
       " 769: 'handle',\n",
       " 770: 'wondered',\n",
       " 771: 'mixed',\n",
       " 772: 'deformed',\n",
       " 773: 'flung',\n",
       " 774: 'iron',\n",
       " 775: 'arched',\n",
       " 776: 'cut',\n",
       " 777: 'times',\n",
       " 778: 'beneath',\n",
       " 779: 'covered',\n",
       " 780: 'gathered',\n",
       " 781: 'stands',\n",
       " 782: 'rude',\n",
       " 783: 'bone',\n",
       " 784: 'abominable',\n",
       " 785: 'measure',\n",
       " 786: 'seamen',\n",
       " 787: 'skrimshander',\n",
       " 788: 'sought',\n",
       " 789: 'added',\n",
       " 790: 'forehead',\n",
       " 791: 'objections',\n",
       " 792: \"'d\",\n",
       " 793: 'used',\n",
       " 794: 'depend',\n",
       " 795: 'decent',\n",
       " 796: 'want',\n",
       " 797: 'ready',\n",
       " 798: 'working',\n",
       " 799: 'space',\n",
       " 800: 'lips',\n",
       " 801: 'fingers',\n",
       " 802: 'fare',\n",
       " 803: 'fellow',\n",
       " 804: 'nightmare',\n",
       " 805: 'nt',\n",
       " 806: 'complexioned',\n",
       " 807: 'eats',\n",
       " 808: \"'em\",\n",
       " 809: 'afore',\n",
       " 810: 'resolved',\n",
       " 811: 'noise',\n",
       " 812: 'offing',\n",
       " 813: 'shaggy',\n",
       " 814: 'woollen',\n",
       " 815: 'stiff',\n",
       " 816: 'labrador',\n",
       " 817: 'wake',\n",
       " 818: 'bad',\n",
       " 819: 'mounted',\n",
       " 820: 'generally',\n",
       " 821: 'shipmates',\n",
       " 822: 'become',\n",
       " 823: 'height',\n",
       " 824: 'seem',\n",
       " 825: 'plan',\n",
       " 826: 'private',\n",
       " 827: 'unknown',\n",
       " 828: 'apartment',\n",
       " 829: 'hammock',\n",
       " 830: 'linen',\n",
       " 831: 'home',\n",
       " 832: 'hole',\n",
       " 833: 'mattress',\n",
       " 834: 'planing',\n",
       " 835: 'knot',\n",
       " 836: 'near',\n",
       " 837: 'sake',\n",
       " 838: 'chair',\n",
       " 839: 'narrow',\n",
       " 840: 'leaving',\n",
       " 841: 'interval',\n",
       " 842: 'met',\n",
       " 843: 'inside',\n",
       " 844: 'violent',\n",
       " 845: 'ones',\n",
       " 846: 'comprehension',\n",
       " 847: 'bird',\n",
       " 848: 'airley',\n",
       " 849: 'airth',\n",
       " 850: 'engaged',\n",
       " 851: 'whittling',\n",
       " 852: 'guess',\n",
       " 853: 'done',\n",
       " 854: 'break',\n",
       " 855: 'broke',\n",
       " 856: 'understand',\n",
       " 857: 'certain',\n",
       " 858: 'demand',\n",
       " 859: 'selling',\n",
       " 860: 'sir',\n",
       " 861: 'string',\n",
       " 862: 'mystery',\n",
       " 863: 'showed',\n",
       " 864: 'dangerous',\n",
       " 865: 'flukes',\n",
       " 866: 'slept',\n",
       " 867: 'big',\n",
       " 868: 'sam',\n",
       " 869: 'lighted',\n",
       " 870: 'wo',\n",
       " 871: 'stairs',\n",
       " 872: 'placing',\n",
       " 873: 'double',\n",
       " 874: 'eyeing',\n",
       " 875: 'belonging',\n",
       " 876: 'papered',\n",
       " 877: 'parcel',\n",
       " 878: 'tried',\n",
       " 879: 'satisfactory',\n",
       " 880: 'concerning',\n",
       " 881: 'edges',\n",
       " 882: 'stuck',\n",
       " 883: 'gave',\n",
       " 884: 'neck',\n",
       " 885: 'sleeves',\n",
       " 886: 'undressed',\n",
       " 887: 'remembering',\n",
       " 888: 'jumped',\n",
       " 889: 'pantaloons',\n",
       " 890: 'blowing',\n",
       " 891: 'doze',\n",
       " 892: 'pretty',\n",
       " 893: 'heavy',\n",
       " 894: 'save',\n",
       " 895: 'infernal',\n",
       " 896: 'peddler',\n",
       " 897: 'yellow',\n",
       " 898: 'colour',\n",
       " 899: 'dreadfully',\n",
       " 900: 'sticking',\n",
       " 901: 'cheeks',\n",
       " 902: 'truth',\n",
       " 903: 'remembered',\n",
       " 904: 'whaleman',\n",
       " 905: 'tattooed',\n",
       " 906: 'concluded',\n",
       " 907: 'lying',\n",
       " 908: 'tanning',\n",
       " 909: 'hot',\n",
       " 910: 'produced',\n",
       " 911: 'beaver',\n",
       " 912: 'singing',\n",
       " 913: 'bolted',\n",
       " 914: 'arms',\n",
       " 915: 'running',\n",
       " 916: 'curious',\n",
       " 917: 'hunch',\n",
       " 918: 'congo',\n",
       " 919: 'ill',\n",
       " 920: 'takes',\n",
       " 921: 'biscuit',\n",
       " 922: 'top',\n",
       " 923: 'lamp',\n",
       " 924: 'succeeded',\n",
       " 925: 'polite',\n",
       " 926: 'guttural',\n",
       " 927: 'pagan',\n",
       " 928: 'spell',\n",
       " 929: 'tobacco',\n",
       " 930: 'grunt',\n",
       " 931: 'whatever',\n",
       " 932: 'ee',\n",
       " 933: 'horrid',\n",
       " 934: 'pipe',\n",
       " 935: 'smoking',\n",
       " 936: 'complied',\n",
       " 937: 'patchwork',\n",
       " 938: 'figure',\n",
       " 939: 'shade',\n",
       " 940: 'quilt',\n",
       " 941: 'hugging',\n",
       " 942: 'sensations',\n",
       " 943: 'explain',\n",
       " 944: 'remember',\n",
       " 945: 'circumstance',\n",
       " 946: 'reality',\n",
       " 947: 'stepmother',\n",
       " 948: 'sixteen',\n",
       " 949: 'abed',\n",
       " 950: 'troubled',\n",
       " 951: 'supernatural',\n",
       " 952: 'ages',\n",
       " 953: 'awful',\n",
       " 954: 'consciousness',\n",
       " 955: 'observing',\n",
       " 956: 'creature',\n",
       " 957: 'dress',\n",
       " 958: 'watching',\n",
       " 959: 'probably',\n",
       " 960: 'toilet',\n",
       " 961: 'wrapped',\n",
       " 962: 'precisely',\n",
       " 963: 'jacket',\n",
       " 964: 'call',\n",
       " 965: 'shore',\n",
       " 966: 'watery',\n",
       " 967: 'driving',\n",
       " 968: 'spleen',\n",
       " 969: 'regulating',\n",
       " 970: 'circulation',\n",
       " 971: 'growing',\n",
       " 972: 'grim',\n",
       " 973: 'drizzly',\n",
       " 974: 'november',\n",
       " 975: 'bringing',\n",
       " 976: 'rear',\n",
       " 977: 'funeral',\n",
       " 978: 'meet',\n",
       " 979: 'hypos',\n",
       " 980: 'upper',\n",
       " 981: 'moral',\n",
       " 982: 'principle',\n",
       " 983: 'prevent',\n",
       " 984: 'deliberately',\n",
       " 985: 'stepping',\n",
       " 986: 'methodically',\n",
       " 987: 'knocking',\n",
       " 988: 'hats',\n",
       " 989: 'substitute',\n",
       " 990: 'pistol',\n",
       " 991: 'ball',\n",
       " 992: 'philosophical',\n",
       " 993: 'flourish',\n",
       " 994: 'cato',\n",
       " 995: 'throws',\n",
       " 996: 'surprising',\n",
       " 997: 'cherish',\n",
       " 998: 'feelings',\n",
       " 999: 'insular',\n",
       " 1000: 'manhattoes',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2709 total different words.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "print(f\"We have {vocabulary_size} total different words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets convert these sequences into a numpy array for better handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 964,   14,  265, ..., 2704,   14,   24],\n",
       "       [  14,  265,   51, ...,   14,   24,  965],\n",
       "       [ 265,   51,  263, ...,   24,  965,    5],\n",
       "       ...,\n",
       "       [ 960,   12,  168, ...,  264,   53,    2],\n",
       "       [  12,  168, 2703, ...,   53,    2, 2709],\n",
       "       [ 168, 2703,    3, ...,    2, 2709,   26]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequences_array = np.array(sequences)\n",
    "sequences_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, we want to predict the next word for a particular sequence, right now our sequences are composed of 26 characters, we want to predict character 26. For this, we need to separate the last column from the rest to have a nice $X$ and $y$ scenario to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# take up everything up until the last column\n",
    "X = sequences_array[:, :-1]\n",
    "# take the last column separately\n",
    "y = sequences_array[:, -1]\n",
    "# and convert the current id based encoding into categorical values (one-hot encoding)\n",
    "# we add one to num_classes because of how keras padding works it needs an additional spot for holding 0\n",
    "y = to_categorical(y, num_classes=vocabulary_size + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's review the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11368, 25)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have 11368 samples with 25 words (dimenssions).\n",
    "\n",
    "## Work with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "s_config = tf.ConfigProto()\n",
    "s_config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "tf.keras.backend.set_session(tf.Session(config=s_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Dropout\n",
    "\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(50, return_sequences=True)) # use some multiple of the sequence length\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(50))\n",
    "    \n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(vocabulary_size, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 25, 25)            67750     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 25, 50)            15200     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 25, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2710)              349590    \n",
      "=================================================================\n",
      "Total params: 459,268\n",
      "Trainable params: 459,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = X.shape[1]\n",
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "11368/11368 [==============================] - 2s 212us/step - loss: 7.8472 - acc: 0.0284\n",
      "Epoch 2/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 6.7076 - acc: 0.0360\n",
      "Epoch 3/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 6.4086 - acc: 0.0510\n",
      "Epoch 4/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 6.3739 - acc: 0.0480\n",
      "Epoch 5/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 6.3625 - acc: 0.0522\n",
      "Epoch 6/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 6.3594 - acc: 0.0506\n",
      "Epoch 7/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 6.3511 - acc: 0.0523\n",
      "Epoch 8/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 6.3503 - acc: 0.0515\n",
      "Epoch 9/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 6.3436 - acc: 0.0518\n",
      "Epoch 10/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 6.3507 - acc: 0.0515\n",
      "Epoch 11/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 6.3506 - acc: 0.0521\n",
      "Epoch 12/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 6.3439 - acc: 0.0530\n",
      "Epoch 13/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 6.3386 - acc: 0.0523\n",
      "Epoch 14/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 6.3356 - acc: 0.0523\n",
      "Epoch 15/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 6.3211 - acc: 0.0529\n",
      "Epoch 16/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 6.2859 - acc: 0.0523\n",
      "Epoch 17/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 6.2237 - acc: 0.0524\n",
      "Epoch 18/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 6.1580 - acc: 0.0529\n",
      "Epoch 19/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 6.1212 - acc: 0.0522\n",
      "Epoch 20/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 6.0905 - acc: 0.0522\n",
      "Epoch 21/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 6.0587 - acc: 0.0523\n",
      "Epoch 22/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 6.0253 - acc: 0.0528\n",
      "Epoch 23/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 5.9989 - acc: 0.0544\n",
      "Epoch 24/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.9700 - acc: 0.0540\n",
      "Epoch 25/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.9458 - acc: 0.0541\n",
      "Epoch 26/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.9172 - acc: 0.0555\n",
      "Epoch 27/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.8883 - acc: 0.0571\n",
      "Epoch 28/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.8701 - acc: 0.0572\n",
      "Epoch 29/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.8393 - acc: 0.0602\n",
      "Epoch 30/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.8242 - acc: 0.0598\n",
      "Epoch 31/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.8096 - acc: 0.0598\n",
      "Epoch 32/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.7846 - acc: 0.0613\n",
      "Epoch 33/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.7627 - acc: 0.0618\n",
      "Epoch 34/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.7481 - acc: 0.0646\n",
      "Epoch 35/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.7332 - acc: 0.0617\n",
      "Epoch 36/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.7089 - acc: 0.0651\n",
      "Epoch 37/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.6779 - acc: 0.0640\n",
      "Epoch 38/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.6440 - acc: 0.0659\n",
      "Epoch 39/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 5.6256 - acc: 0.0666\n",
      "Epoch 40/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.5912 - acc: 0.0667\n",
      "Epoch 41/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.5660 - acc: 0.0675\n",
      "Epoch 42/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.5465 - acc: 0.0691\n",
      "Epoch 43/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.5195 - acc: 0.0677\n",
      "Epoch 44/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.5104 - acc: 0.0669\n",
      "Epoch 45/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.4745 - acc: 0.0705\n",
      "Epoch 46/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.4405 - acc: 0.0691\n",
      "Epoch 47/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.4138 - acc: 0.0704\n",
      "Epoch 48/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.3980 - acc: 0.0716\n",
      "Epoch 49/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 5.3799 - acc: 0.0702\n",
      "Epoch 50/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.3644 - acc: 0.0715\n",
      "Epoch 51/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 5.3416 - acc: 0.0711\n",
      "Epoch 52/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.3301 - acc: 0.0736\n",
      "Epoch 53/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.3074 - acc: 0.0737\n",
      "Epoch 54/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.2916 - acc: 0.0737\n",
      "Epoch 55/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.2644 - acc: 0.0730\n",
      "Epoch 56/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.2436 - acc: 0.0749\n",
      "Epoch 57/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.2271 - acc: 0.0738\n",
      "Epoch 58/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.2134 - acc: 0.0735\n",
      "Epoch 59/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 5.1896 - acc: 0.0733\n",
      "Epoch 60/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 5.1781 - acc: 0.0760\n",
      "Epoch 61/500\n",
      "11368/11368 [==============================] - 1s 95us/step - loss: 5.1606 - acc: 0.0769\n",
      "Epoch 62/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 5.1401 - acc: 0.0766\n",
      "Epoch 63/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 5.1211 - acc: 0.0769\n",
      "Epoch 64/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 5.1129 - acc: 0.0773\n",
      "Epoch 65/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 5.1011 - acc: 0.0780\n",
      "Epoch 66/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.0877 - acc: 0.0801\n",
      "Epoch 67/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.0831 - acc: 0.0789\n",
      "Epoch 68/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.0727 - acc: 0.0763\n",
      "Epoch 69/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.0571 - acc: 0.0811\n",
      "Epoch 70/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.0440 - acc: 0.0808\n",
      "Epoch 71/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.0244 - acc: 0.0807\n",
      "Epoch 72/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 5.0252 - acc: 0.0757\n",
      "Epoch 73/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 5.0156 - acc: 0.0795\n",
      "Epoch 74/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.9905 - acc: 0.0798\n",
      "Epoch 75/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.9838 - acc: 0.0796\n",
      "Epoch 76/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.9725 - acc: 0.0832\n",
      "Epoch 77/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.9552 - acc: 0.0808\n",
      "Epoch 78/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 4.9650 - acc: 0.0822\n",
      "Epoch 79/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 4.9398 - acc: 0.0831\n",
      "Epoch 80/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.9296 - acc: 0.0785\n",
      "Epoch 81/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.9280 - acc: 0.0798\n",
      "Epoch 82/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 4.9036 - acc: 0.0827\n",
      "Epoch 83/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.8971 - acc: 0.0815\n",
      "Epoch 84/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.8949 - acc: 0.0826\n",
      "Epoch 85/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.8726 - acc: 0.0816\n",
      "Epoch 86/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 4.8537 - acc: 0.0844\n",
      "Epoch 87/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.8538 - acc: 0.0845\n",
      "Epoch 88/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.8493 - acc: 0.0838\n",
      "Epoch 89/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.8295 - acc: 0.0856\n",
      "Epoch 90/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.8340 - acc: 0.0836\n",
      "Epoch 91/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.8101 - acc: 0.0846\n",
      "Epoch 92/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.8079 - acc: 0.0850\n",
      "Epoch 93/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.7882 - acc: 0.0891\n",
      "Epoch 94/500\n",
      "11368/11368 [==============================] - 1s 97us/step - loss: 4.7858 - acc: 0.0869\n",
      "Epoch 95/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.7795 - acc: 0.0895\n",
      "Epoch 96/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 4.7738 - acc: 0.0874\n",
      "Epoch 97/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 4.7465 - acc: 0.0849\n",
      "Epoch 98/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 4.7400 - acc: 0.0892\n",
      "Epoch 99/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.7413 - acc: 0.0883\n",
      "Epoch 100/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.7203 - acc: 0.0894\n",
      "Epoch 101/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 4.7091 - acc: 0.0925\n",
      "Epoch 102/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.7138 - acc: 0.0898\n",
      "Epoch 103/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.7062 - acc: 0.0887\n",
      "Epoch 104/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 4.6998 - acc: 0.0859\n",
      "Epoch 105/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.6783 - acc: 0.0898\n",
      "Epoch 106/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.6698 - acc: 0.0913\n",
      "Epoch 107/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.6553 - acc: 0.0905\n",
      "Epoch 108/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.6401 - acc: 0.0910\n",
      "Epoch 109/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.6346 - acc: 0.0916\n",
      "Epoch 110/500\n",
      "11368/11368 [==============================] - 1s 100us/step - loss: 4.6320 - acc: 0.0934\n",
      "Epoch 111/500\n",
      "11368/11368 [==============================] - 1s 94us/step - loss: 4.6093 - acc: 0.0921\n",
      "Epoch 112/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.6149 - acc: 0.0936\n",
      "Epoch 113/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.6038 - acc: 0.0924\n",
      "Epoch 114/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 4.5937 - acc: 0.0963\n",
      "Epoch 115/500\n",
      "11368/11368 [==============================] - 1s 97us/step - loss: 4.5758 - acc: 0.0936\n",
      "Epoch 116/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.5751 - acc: 0.0969\n",
      "Epoch 117/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.5582 - acc: 0.0950\n",
      "Epoch 118/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.5533 - acc: 0.0962\n",
      "Epoch 119/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.5533 - acc: 0.0972\n",
      "Epoch 120/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.5347 - acc: 0.0980\n",
      "Epoch 121/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.5175 - acc: 0.0969\n",
      "Epoch 122/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.5169 - acc: 0.0979\n",
      "Epoch 123/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.5111 - acc: 0.0979\n",
      "Epoch 124/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.4942 - acc: 0.1006\n",
      "Epoch 125/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.4771 - acc: 0.1001\n",
      "Epoch 126/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.4863 - acc: 0.1011\n",
      "Epoch 127/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.4599 - acc: 0.1016\n",
      "Epoch 128/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.4481 - acc: 0.0991\n",
      "Epoch 129/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.4459 - acc: 0.1034\n",
      "Epoch 130/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 4.4555 - acc: 0.0976\n",
      "Epoch 131/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.4511 - acc: 0.1041\n",
      "Epoch 132/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.4150 - acc: 0.1026\n",
      "Epoch 133/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.3993 - acc: 0.1034\n",
      "Epoch 134/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.3937 - acc: 0.1014\n",
      "Epoch 135/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.3854 - acc: 0.1057\n",
      "Epoch 136/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.3649 - acc: 0.1040\n",
      "Epoch 137/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.3538 - acc: 0.1082\n",
      "Epoch 138/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.3514 - acc: 0.1062\n",
      "Epoch 139/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 4.3413 - acc: 0.1073\n",
      "Epoch 140/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.3317 - acc: 0.1044\n",
      "Epoch 141/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 4.3156 - acc: 0.1084\n",
      "Epoch 142/500\n",
      "11368/11368 [==============================] - 1s 83us/step - loss: 4.3115 - acc: 0.1088\n",
      "Epoch 143/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.2923 - acc: 0.1084\n",
      "Epoch 144/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.2831 - acc: 0.1104\n",
      "Epoch 145/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 4.2855 - acc: 0.1135\n",
      "Epoch 146/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.2768 - acc: 0.1107\n",
      "Epoch 147/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 4.2712 - acc: 0.1103\n",
      "Epoch 148/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 4.2493 - acc: 0.1110\n",
      "Epoch 149/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 4.2391 - acc: 0.1163\n",
      "Epoch 150/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 4.2289 - acc: 0.1137\n",
      "Epoch 151/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 4.2181 - acc: 0.1150\n",
      "Epoch 152/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 4.2112 - acc: 0.1132\n",
      "Epoch 153/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.2097 - acc: 0.1143\n",
      "Epoch 154/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.1940 - acc: 0.1223\n",
      "Epoch 155/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.1721 - acc: 0.1187\n",
      "Epoch 156/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 4.1682 - acc: 0.1167\n",
      "Epoch 157/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.1515 - acc: 0.1188\n",
      "Epoch 158/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.1492 - acc: 0.1257\n",
      "Epoch 159/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.1608 - acc: 0.1192\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.1484 - acc: 0.1226\n",
      "Epoch 161/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.1202 - acc: 0.1272\n",
      "Epoch 162/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.1118 - acc: 0.1282\n",
      "Epoch 163/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.0928 - acc: 0.1217\n",
      "Epoch 164/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.0833 - acc: 0.1233\n",
      "Epoch 165/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 4.0816 - acc: 0.1245\n",
      "Epoch 166/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.0571 - acc: 0.1260\n",
      "Epoch 167/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.0737 - acc: 0.1283\n",
      "Epoch 168/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.0583 - acc: 0.1290\n",
      "Epoch 169/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.0449 - acc: 0.1301\n",
      "Epoch 170/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.0478 - acc: 0.1291\n",
      "Epoch 171/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.0207 - acc: 0.1290\n",
      "Epoch 172/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 4.0102 - acc: 0.1332\n",
      "Epoch 173/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 4.0076 - acc: 0.1342\n",
      "Epoch 174/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 4.0049 - acc: 0.1302\n",
      "Epoch 175/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.9814 - acc: 0.1368\n",
      "Epoch 176/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.9700 - acc: 0.1398\n",
      "Epoch 177/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.9737 - acc: 0.1368\n",
      "Epoch 178/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.9642 - acc: 0.1394\n",
      "Epoch 179/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.9615 - acc: 0.1366\n",
      "Epoch 180/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.9558 - acc: 0.1335\n",
      "Epoch 181/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.9370 - acc: 0.1448\n",
      "Epoch 182/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.9186 - acc: 0.1401\n",
      "Epoch 183/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.9144 - acc: 0.1422\n",
      "Epoch 184/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.8889 - acc: 0.1444\n",
      "Epoch 185/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.8991 - acc: 0.1417\n",
      "Epoch 186/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.8785 - acc: 0.1403\n",
      "Epoch 187/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.8906 - acc: 0.1429\n",
      "Epoch 188/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.8854 - acc: 0.1458\n",
      "Epoch 189/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.8692 - acc: 0.1435\n",
      "Epoch 190/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.8494 - acc: 0.1499\n",
      "Epoch 191/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.8546 - acc: 0.1476\n",
      "Epoch 192/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.8451 - acc: 0.1491\n",
      "Epoch 193/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.8257 - acc: 0.1500\n",
      "Epoch 194/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.8216 - acc: 0.1530\n",
      "Epoch 195/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.8194 - acc: 0.1480\n",
      "Epoch 196/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.8204 - acc: 0.1514\n",
      "Epoch 197/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.8112 - acc: 0.1500\n",
      "Epoch 198/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.7979 - acc: 0.1522\n",
      "Epoch 199/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.7754 - acc: 0.1551\n",
      "Epoch 200/500\n",
      "11368/11368 [==============================] - 1s 83us/step - loss: 3.7740 - acc: 0.1600\n",
      "Epoch 201/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.7777 - acc: 0.1613\n",
      "Epoch 202/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.7587 - acc: 0.1597\n",
      "Epoch 203/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.7537 - acc: 0.1616\n",
      "Epoch 204/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.7467 - acc: 0.1601\n",
      "Epoch 205/500\n",
      "11368/11368 [==============================] - 1s 83us/step - loss: 3.7284 - acc: 0.1606\n",
      "Epoch 206/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.7170 - acc: 0.1653\n",
      "Epoch 207/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.7236 - acc: 0.1643\n",
      "Epoch 208/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.7175 - acc: 0.1650\n",
      "Epoch 209/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.6990 - acc: 0.1656\n",
      "Epoch 210/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.6914 - acc: 0.1664\n",
      "Epoch 211/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.6870 - acc: 0.1629\n",
      "Epoch 212/500\n",
      "11368/11368 [==============================] - 1s 83us/step - loss: 3.7039 - acc: 0.1632\n",
      "Epoch 213/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.6818 - acc: 0.1687\n",
      "Epoch 214/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.6694 - acc: 0.1668\n",
      "Epoch 215/500\n",
      "11368/11368 [==============================] - 1s 83us/step - loss: 3.6528 - acc: 0.1678\n",
      "Epoch 216/500\n",
      "11368/11368 [==============================] - 1s 83us/step - loss: 3.6333 - acc: 0.1757\n",
      "Epoch 217/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 3.6493 - acc: 0.1712\n",
      "Epoch 218/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 3.6396 - acc: 0.1719\n",
      "Epoch 219/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.6259 - acc: 0.1724\n",
      "Epoch 220/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 3.6236 - acc: 0.1700\n",
      "Epoch 221/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 3.6126 - acc: 0.1731\n",
      "Epoch 222/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.6116 - acc: 0.1686\n",
      "Epoch 223/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.5949 - acc: 0.1775\n",
      "Epoch 224/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.6075 - acc: 0.1792\n",
      "Epoch 225/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.5943 - acc: 0.1767\n",
      "Epoch 226/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.5849 - acc: 0.1791\n",
      "Epoch 227/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.5782 - acc: 0.1871\n",
      "Epoch 228/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.5620 - acc: 0.1854\n",
      "Epoch 229/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.5543 - acc: 0.1826\n",
      "Epoch 230/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.5478 - acc: 0.1841\n",
      "Epoch 231/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.5547 - acc: 0.1839\n",
      "Epoch 232/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 3.5376 - acc: 0.1859\n",
      "Epoch 233/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.5378 - acc: 0.1860\n",
      "Epoch 234/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.5139 - acc: 0.1918\n",
      "Epoch 235/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.5161 - acc: 0.1917\n",
      "Epoch 236/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 3.5134 - acc: 0.1877\n",
      "Epoch 237/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.5176 - acc: 0.1868\n",
      "Epoch 238/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.4936 - acc: 0.1972\n",
      "Epoch 239/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.4992 - acc: 0.1877\n",
      "Epoch 240/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 3.4825 - acc: 0.1915\n",
      "Epoch 241/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 3.4765 - acc: 0.1919\n",
      "Epoch 242/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.4493 - acc: 0.1939\n",
      "Epoch 243/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.4627 - acc: 0.1961\n",
      "Epoch 244/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.4505 - acc: 0.1990\n",
      "Epoch 245/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 3.4610 - acc: 0.1928\n",
      "Epoch 246/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.4454 - acc: 0.1965\n",
      "Epoch 247/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.4487 - acc: 0.1961\n",
      "Epoch 248/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.4379 - acc: 0.2034\n",
      "Epoch 249/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.4219 - acc: 0.1967\n",
      "Epoch 250/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.4094 - acc: 0.2015\n",
      "Epoch 251/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.3988 - acc: 0.2012\n",
      "Epoch 252/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.4094 - acc: 0.2019\n",
      "Epoch 253/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.4094 - acc: 0.2020\n",
      "Epoch 254/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.3947 - acc: 0.2085\n",
      "Epoch 255/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.3866 - acc: 0.2065\n",
      "Epoch 256/500\n",
      "11368/11368 [==============================] - 1s 97us/step - loss: 3.3902 - acc: 0.1994\n",
      "Epoch 257/500\n",
      "11368/11368 [==============================] - 1s 106us/step - loss: 3.3849 - acc: 0.2095\n",
      "Epoch 258/500\n",
      "11368/11368 [==============================] - 1s 100us/step - loss: 3.3566 - acc: 0.2082\n",
      "Epoch 259/500\n",
      "11368/11368 [==============================] - 1s 100us/step - loss: 3.3557 - acc: 0.2133\n",
      "Epoch 260/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.3512 - acc: 0.2108\n",
      "Epoch 261/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.3452 - acc: 0.2169\n",
      "Epoch 262/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.3408 - acc: 0.2105\n",
      "Epoch 263/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.3350 - acc: 0.2143\n",
      "Epoch 264/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.3274 - acc: 0.2119\n",
      "Epoch 265/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.3064 - acc: 0.2165\n",
      "Epoch 266/500\n",
      "11368/11368 [==============================] - 1s 93us/step - loss: 3.3296 - acc: 0.2169\n",
      "Epoch 267/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.3062 - acc: 0.2174\n",
      "Epoch 268/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.3129 - acc: 0.2170\n",
      "Epoch 269/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.2946 - acc: 0.2214\n",
      "Epoch 270/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.2753 - acc: 0.2314\n",
      "Epoch 271/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.2787 - acc: 0.2272\n",
      "Epoch 272/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.2735 - acc: 0.2214\n",
      "Epoch 273/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 3.2612 - acc: 0.2266\n",
      "Epoch 274/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.2657 - acc: 0.2220\n",
      "Epoch 275/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.2571 - acc: 0.2214\n",
      "Epoch 276/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.2426 - acc: 0.2249\n",
      "Epoch 277/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.2561 - acc: 0.2269\n",
      "Epoch 278/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.2731 - acc: 0.2262\n",
      "Epoch 279/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.2425 - acc: 0.2288\n",
      "Epoch 280/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.2608 - acc: 0.2254\n",
      "Epoch 281/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 3.2286 - acc: 0.2293\n",
      "Epoch 282/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.2189 - acc: 0.2348\n",
      "Epoch 283/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.2004 - acc: 0.2352\n",
      "Epoch 284/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.2018 - acc: 0.2343\n",
      "Epoch 285/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.2060 - acc: 0.2361\n",
      "Epoch 286/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1878 - acc: 0.2387\n",
      "Epoch 287/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.1884 - acc: 0.2365\n",
      "Epoch 288/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1842 - acc: 0.2356\n",
      "Epoch 289/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.1769 - acc: 0.2372\n",
      "Epoch 290/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1907 - acc: 0.2333\n",
      "Epoch 291/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1706 - acc: 0.2382\n",
      "Epoch 292/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1614 - acc: 0.2483\n",
      "Epoch 293/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.1498 - acc: 0.2446\n",
      "Epoch 294/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.1546 - acc: 0.2472\n",
      "Epoch 295/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.1352 - acc: 0.2462\n",
      "Epoch 296/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.1419 - acc: 0.2446\n",
      "Epoch 297/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1422 - acc: 0.2482\n",
      "Epoch 298/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.1241 - acc: 0.2469\n",
      "Epoch 299/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1330 - acc: 0.2497\n",
      "Epoch 300/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1049 - acc: 0.2477\n",
      "Epoch 301/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.1122 - acc: 0.2468\n",
      "Epoch 302/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 3.1040 - acc: 0.2505\n",
      "Epoch 303/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.1263 - acc: 0.2474\n",
      "Epoch 304/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.1092 - acc: 0.2497\n",
      "Epoch 305/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.0952 - acc: 0.2505\n",
      "Epoch 306/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.0741 - acc: 0.2558\n",
      "Epoch 307/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.0877 - acc: 0.2557\n",
      "Epoch 308/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.0712 - acc: 0.2602\n",
      "Epoch 309/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.0732 - acc: 0.2521\n",
      "Epoch 310/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.0597 - acc: 0.2633\n",
      "Epoch 311/500\n",
      "11368/11368 [==============================] - 1s 92us/step - loss: 3.0602 - acc: 0.2574\n",
      "Epoch 312/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 3.0558 - acc: 0.2555\n",
      "Epoch 313/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.0578 - acc: 0.2540\n",
      "Epoch 314/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.0437 - acc: 0.2622\n",
      "Epoch 315/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.0286 - acc: 0.2640\n",
      "Epoch 316/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.0483 - acc: 0.2649\n",
      "Epoch 317/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.0416 - acc: 0.2635\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.0135 - acc: 0.2660\n",
      "Epoch 319/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.0337 - acc: 0.2610\n",
      "Epoch 320/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.0204 - acc: 0.2650\n",
      "Epoch 321/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.0224 - acc: 0.2639\n",
      "Epoch 322/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 3.0149 - acc: 0.2621\n",
      "Epoch 323/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 3.0001 - acc: 0.2758\n",
      "Epoch 324/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 3.0018 - acc: 0.2649\n",
      "Epoch 325/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 3.0005 - acc: 0.2696\n",
      "Epoch 326/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9751 - acc: 0.2722\n",
      "Epoch 327/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.9931 - acc: 0.2679\n",
      "Epoch 328/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9713 - acc: 0.2801\n",
      "Epoch 329/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 2.9668 - acc: 0.2742\n",
      "Epoch 330/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9588 - acc: 0.2760\n",
      "Epoch 331/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9761 - acc: 0.2744\n",
      "Epoch 332/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.9682 - acc: 0.2781\n",
      "Epoch 333/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 2.9629 - acc: 0.2734\n",
      "Epoch 334/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9391 - acc: 0.2813\n",
      "Epoch 335/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9261 - acc: 0.2815\n",
      "Epoch 336/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 2.9464 - acc: 0.2796\n",
      "Epoch 337/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.9484 - acc: 0.2789\n",
      "Epoch 338/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.9350 - acc: 0.2858\n",
      "Epoch 339/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.9281 - acc: 0.2821\n",
      "Epoch 340/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9154 - acc: 0.2839\n",
      "Epoch 341/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9307 - acc: 0.2774\n",
      "Epoch 342/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.9168 - acc: 0.2789\n",
      "Epoch 343/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.9002 - acc: 0.2879\n",
      "Epoch 344/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8973 - acc: 0.2826\n",
      "Epoch 345/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8960 - acc: 0.2811\n",
      "Epoch 346/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.9249 - acc: 0.2814\n",
      "Epoch 347/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.9072 - acc: 0.2876\n",
      "Epoch 348/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.9044 - acc: 0.2835\n",
      "Epoch 349/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 2.8845 - acc: 0.2896\n",
      "Epoch 350/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8643 - acc: 0.2906\n",
      "Epoch 351/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.8765 - acc: 0.2907\n",
      "Epoch 352/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8517 - acc: 0.2934\n",
      "Epoch 353/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8515 - acc: 0.2957\n",
      "Epoch 354/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.8702 - acc: 0.2871\n",
      "Epoch 355/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8626 - acc: 0.2921\n",
      "Epoch 356/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8514 - acc: 0.2952\n",
      "Epoch 357/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.8682 - acc: 0.2926\n",
      "Epoch 358/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.8393 - acc: 0.2990\n",
      "Epoch 359/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.8264 - acc: 0.2994\n",
      "Epoch 360/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.8303 - acc: 0.2966\n",
      "Epoch 361/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.8217 - acc: 0.3010\n",
      "Epoch 362/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 2.8182 - acc: 0.3006\n",
      "Epoch 363/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 2.8029 - acc: 0.3045\n",
      "Epoch 364/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.8216 - acc: 0.3003\n",
      "Epoch 365/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 2.8053 - acc: 0.3021\n",
      "Epoch 366/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 2.8023 - acc: 0.3045\n",
      "Epoch 367/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 2.7853 - acc: 0.3096\n",
      "Epoch 368/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.8125 - acc: 0.3008\n",
      "Epoch 369/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 2.8052 - acc: 0.3046\n",
      "Epoch 370/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8040 - acc: 0.3027\n",
      "Epoch 371/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 2.8113 - acc: 0.3013\n",
      "Epoch 372/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.7926 - acc: 0.3045\n",
      "Epoch 373/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.7845 - acc: 0.3068\n",
      "Epoch 374/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.8093 - acc: 0.3043\n",
      "Epoch 375/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.7708 - acc: 0.3059\n",
      "Epoch 376/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.7741 - acc: 0.3066\n",
      "Epoch 377/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.7585 - acc: 0.3061\n",
      "Epoch 378/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.7525 - acc: 0.3132\n",
      "Epoch 379/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.7422 - acc: 0.3140\n",
      "Epoch 380/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.7560 - acc: 0.3102\n",
      "Epoch 381/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.7401 - acc: 0.3117\n",
      "Epoch 382/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.7458 - acc: 0.3088\n",
      "Epoch 383/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.7296 - acc: 0.3156\n",
      "Epoch 384/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.7334 - acc: 0.3146\n",
      "Epoch 385/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.7250 - acc: 0.3175\n",
      "Epoch 386/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.7328 - acc: 0.3162\n",
      "Epoch 387/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 2.7181 - acc: 0.3127\n",
      "Epoch 388/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.7194 - acc: 0.3143\n",
      "Epoch 389/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.7338 - acc: 0.3141\n",
      "Epoch 390/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 2.7138 - acc: 0.3179\n",
      "Epoch 391/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 2.7100 - acc: 0.3237\n",
      "Epoch 392/500\n",
      "11368/11368 [==============================] - 1s 90us/step - loss: 2.7046 - acc: 0.3196\n",
      "Epoch 393/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 2.6976 - acc: 0.3163\n",
      "Epoch 394/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.6907 - acc: 0.3220\n",
      "Epoch 395/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 2.7017 - acc: 0.3186\n",
      "Epoch 396/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 2.7086 - acc: 0.3176\n",
      "Epoch 397/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 2.6652 - acc: 0.3273\n",
      "Epoch 398/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 2.6817 - acc: 0.3219\n",
      "Epoch 399/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6738 - acc: 0.3306\n",
      "Epoch 400/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.6809 - acc: 0.3230\n",
      "Epoch 401/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.6633 - acc: 0.3285\n",
      "Epoch 402/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6749 - acc: 0.3267\n",
      "Epoch 403/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6942 - acc: 0.3185\n",
      "Epoch 404/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.6619 - acc: 0.3316\n",
      "Epoch 405/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.6491 - acc: 0.3299\n",
      "Epoch 406/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6443 - acc: 0.3342\n",
      "Epoch 407/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.6591 - acc: 0.3316\n",
      "Epoch 408/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.6529 - acc: 0.3289\n",
      "Epoch 409/500\n",
      "11368/11368 [==============================] - 1s 88us/step - loss: 2.6434 - acc: 0.3293\n",
      "Epoch 410/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6502 - acc: 0.3326\n",
      "Epoch 411/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6333 - acc: 0.3297\n",
      "Epoch 412/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.6380 - acc: 0.3332\n",
      "Epoch 413/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6384 - acc: 0.3301\n",
      "Epoch 414/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.6093 - acc: 0.3330\n",
      "Epoch 415/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6216 - acc: 0.3337\n",
      "Epoch 416/500\n",
      "11368/11368 [==============================] - 1s 83us/step - loss: 2.6076 - acc: 0.3325\n",
      "Epoch 417/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.6118 - acc: 0.3336\n",
      "Epoch 418/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.6119 - acc: 0.3308\n",
      "Epoch 419/500\n",
      "11368/11368 [==============================] - 1s 83us/step - loss: 2.6077 - acc: 0.3392\n",
      "Epoch 420/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.5980 - acc: 0.3361\n",
      "Epoch 421/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.5778 - acc: 0.3395\n",
      "Epoch 422/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 2.5754 - acc: 0.3419\n",
      "Epoch 423/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.5827 - acc: 0.3407\n",
      "Epoch 424/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.5821 - acc: 0.3403\n",
      "Epoch 425/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.5877 - acc: 0.3433\n",
      "Epoch 426/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.5773 - acc: 0.3439\n",
      "Epoch 427/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 2.5599 - acc: 0.3439\n",
      "Epoch 428/500\n",
      "11368/11368 [==============================] - 1s 89us/step - loss: 2.5819 - acc: 0.3459\n",
      "Epoch 429/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 2.5829 - acc: 0.3379\n",
      "Epoch 430/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.5780 - acc: 0.3451\n",
      "Epoch 431/500\n",
      "11368/11368 [==============================] - 1s 91us/step - loss: 2.5588 - acc: 0.3486\n",
      "Epoch 432/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.5631 - acc: 0.3452\n",
      "Epoch 433/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.5701 - acc: 0.3430\n",
      "Epoch 434/500\n",
      "11368/11368 [==============================] - 1s 85us/step - loss: 2.5521 - acc: 0.3483\n",
      "Epoch 435/500\n",
      "11368/11368 [==============================] - 1s 84us/step - loss: 2.5552 - acc: 0.3507\n",
      "Epoch 436/500\n",
      "11368/11368 [==============================] - 1s 87us/step - loss: 2.5609 - acc: 0.3445\n",
      "Epoch 437/500\n",
      "11368/11368 [==============================] - 1s 86us/step - loss: 2.5520 - acc: 0.3481\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=0.01)\n",
    "history = model.fit(X, y, batch_size=512, epochs=500, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "\n",
    "# This will save the weights of the network only\n",
    "model.save(\"models/text-generation.h5\")\n",
    "# This will save the architecture definition as a yaml file\n",
    "with open(\"models/text-generation-def.yaml\", \"w\") as file:\n",
    "    yaml = model.to_yaml()\n",
    "    file.write(yaml)\n",
    "\n",
    "# Finally let's save the tokenizer\n",
    "with open(\"models/text-generation-tokenizer.pkl\", \"wb\") as file:\n",
    "    dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # This will chop off the first word in case the sequence is longer than \n",
    "        # the seq_len\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        pred_word_index = model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        pred_word = tokenizer.index_word[pred_word_index]\n",
    "        \n",
    "        input_text += ' ' + pred_word\n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with my with with that that now then business and warehouses now washed with the lamp and was it myself and a unearthly complexion that'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, seq_len, \"nothing particular to interest me on\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
