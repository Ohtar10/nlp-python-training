{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Bot\n",
    "\n",
    "In this notebook we have an implementation of the ChatBot using the End to End memory networks implementation.\n",
    "\n",
    "First, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "\n",
    "with open('../../datasets/train_qa.txt', 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "    \n",
    "with open('../../datasets/test_qa.txt', 'rb') as file:\n",
    "    test_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data count: 10000\n",
      "Test data count: 1000\n"
     ]
    }
   ],
   "source": [
    "train_count = len(train_data)\n",
    "test_count = len(test_data)\n",
    "print(f\"Train data count: {train_count}\")\n",
    "print(f\"Test data count: {test_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the kind of data we can find in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def print_record(data, index=None):\n",
    "    if not index:\n",
    "        index = random.randint(0, len(data) - 1)\n",
    "    record = data[index]\n",
    "    story = \" \".join(record[0])\n",
    "    story = [s.strip() for s in story.split(\".\")]\n",
    "    story = \"\\n\".join(story)\n",
    "    question = \" \".join(record[1])\n",
    "    answer = record[2]\n",
    "    print(f\"Story:\\n{story}\")\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story:\n",
      "Daniel went to the kitchen\n",
      "John went back to the hallway\n",
      "Sandra travelled to the office\n",
      "John went back to the kitchen\n",
      "\n",
      "Question: Is Sandra in the office ?\n",
      "\n",
      "Answer: yes\n"
     ]
    }
   ],
   "source": [
    "print_record(train_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story:\n",
      "Mary travelled to the office\n",
      "John went to the bathroom\n",
      "John got the football there\n",
      "Mary went back to the hallway\n",
      "\n",
      "Question: Is Mary in the bedroom ?\n",
      "\n",
      "Answer: no\n"
     ]
    }
   ],
   "source": [
    "print_record(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure both the train and test data are used to create a vocabulary, this is to ensure the dictionary contains the elements used in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data + test_data\n",
    "\n",
    "# find all distinct elements between all the words\n",
    "vocab = set()\n",
    "for story, question, answer in all_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))\n",
    "    \n",
    "vocab.add(\"yes\")\n",
    "vocab.add(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len = len(vocab) + 1\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the longest story and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_story_len = [len(data[0]) for data in all_data]\n",
    "max_story_len = max(all_story_len)\n",
    "all_question_len = [len(data[2]) for data in all_data]\n",
    "max_question_len = max(all_question_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'put',\n",
       " 3: 'no',\n",
       " 4: 'moved',\n",
       " 5: 'journeyed',\n",
       " 6: 'travelled',\n",
       " 7: 'football',\n",
       " 8: '.',\n",
       " 9: 'yes',\n",
       " 10: 'discarded',\n",
       " 11: 'picked',\n",
       " 12: 'is',\n",
       " 13: 'garden',\n",
       " 14: 'up',\n",
       " 15: 'daniel',\n",
       " 16: 'got',\n",
       " 17: 'down',\n",
       " 18: 'in',\n",
       " 19: 'grabbed',\n",
       " 20: 'took',\n",
       " 21: 'john',\n",
       " 22: 'left',\n",
       " 23: 'mary',\n",
       " 24: 'bathroom',\n",
       " 25: 'dropped',\n",
       " 26: 'went',\n",
       " 27: 'milk',\n",
       " 28: 'to',\n",
       " 29: 'apple',\n",
       " 30: 'sandra',\n",
       " 31: 'kitchen',\n",
       " 32: 'there',\n",
       " 33: 'bedroom',\n",
       " 34: '?',\n",
       " 35: 'hallway',\n",
       " 36: 'back',\n",
       " 37: 'office'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story, question, answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    train_answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_stories(data, word_index, max_story_length, max_question_length):\n",
    "    # stories\n",
    "    X = []\n",
    "    # questions\n",
    "    Xq = []\n",
    "    # answers\n",
    "    Y = []\n",
    "    for story, question, answer in data:\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        xq = [word_index[word.lower()] for word in question]\n",
    "        \n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        \n",
    "    return (pad_sequences(X, maxlen=max_story_length), pad_sequences(Xq, maxlen=max_question_length), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Xq, y = vectorize_stories(train_data, tokenizer.word_index, max_story_len, max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  1, 33,  8],\n",
       "       [ 0,  0,  0, ...,  1, 35,  8],\n",
       "       [ 0,  0,  0, ...,  1, 24,  8],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  1, 33,  8],\n",
       "       [ 0,  0,  0, ..., 27, 32,  8],\n",
       "       [ 0,  0,  0, ..., 29, 32,  8]], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
